# Data Extractor

ä¸€ä¸ªåŸºäº Scrapy å’Œ FastMCP æ„å»ºçš„å¼ºå¤§ã€ç¨³å®šçš„ç½‘é¡µçˆ¬å–ä¸æ•°æ®æå– MCP Serverï¼Œä¸“ä¸ºå•†ä¸šç¯å¢ƒä¸­çš„é•¿æœŸä½¿ç”¨è€Œè®¾è®¡ã€‚

## ğŸ› ï¸ MCP Server æ ¸å¿ƒå·¥å…· (14 ä¸ª)

| å·¥å…·åç§°                               | åŠŸèƒ½æè¿°           | ä½¿ç”¨åœºæ™¯                          |
| -------------------------------------- | ------------------ | --------------------------------- |
| **scrape_webpage**                     | å•é¡µé¢æŠ“å–         | åŸºç¡€æ•°æ®æå–ï¼Œæ”¯æŒé…ç½®åŒ–é€‰æ‹©å™¨    |
| **scrape_multiple_webpages**           | æ‰¹é‡é¡µé¢æŠ“å–       | å¹¶å‘å¤„ç†å¤šä¸ª URLï¼Œæå‡æ•ˆç‡        |
| **scrape_with_stealth**                | åæ£€æµ‹æŠ“å–         | åº”å¯¹åçˆ¬è™«ä¿æŠ¤çš„é«˜éš¾åº¦ç½‘ç«™        |
| **fill_and_submit_form**               | è¡¨å•è‡ªåŠ¨åŒ–         | ç™»å½•è¡¨å•ã€è”ç³»è¡¨å•ç­‰äº¤äº’æ“ä½œ      |
| **extract_links**                      | ä¸“ä¸šé“¾æ¥æå–       | ç½‘ç«™åœ°å›¾ç”Ÿæˆï¼Œé“¾æ¥åˆ†æ            |
| **extract_structured_data**            | ç»“æ„åŒ–æ•°æ®æå–     | JSON-LDã€å¾®æ•°æ®ã€Open Graph è§£æ  |
| **get_page_info**                      | é¡µé¢ä¿¡æ¯è·å–       | å¿«é€Ÿè·å–æ ‡é¢˜ã€çŠ¶æ€ç ã€å…ƒæ•°æ®      |
| **check_robots_txt**                   | çˆ¬è™«è§„åˆ™æ£€æŸ¥       | éµå®ˆç½‘ç«™çˆ¬å–è§„èŒƒï¼Œåˆè§„æ€§æ£€æŸ¥      |
| **get_server_metrics**                 | æ€§èƒ½æŒ‡æ ‡ç›‘æ§       | æœåŠ¡å™¨çŠ¶æ€ç›‘æ§ï¼Œæ€§èƒ½è°ƒä¼˜          |
| **clear_cache**                        | ç¼“å­˜ç®¡ç†           | é‡Šæ”¾å†…å­˜ï¼Œæ¸…ç†è¿‡æœŸæ•°æ®            |
| **convert_webpage_to_markdown**        | é¡µé¢è½¬ Markdown    | å°†ç½‘é¡µå†…å®¹è½¬æ¢ä¸º Markdown æ ¼å¼    |
| **batch_convert_webpages_to_markdown** | æ‰¹é‡ Markdown è½¬æ¢ | æ‰¹é‡å¤„ç†å¤šä¸ªç½‘é¡µçš„ Markdown è½¬æ¢  |
| **convert_pdf_to_markdown**            | PDF è½¬ Markdown    | å°† PDF æ–‡æ¡£è½¬æ¢ä¸º Markdown æ ¼å¼   |
| **batch_convert_pdfs_to_markdown**     | æ‰¹é‡ PDF è½¬æ¢      | æ‰¹é‡å¤„ç†å¤šä¸ª PDF çš„ Markdown è½¬æ¢ |

### æ ¸å¿ƒåŠŸèƒ½

- **å¤šç§çˆ¬å–æ–¹æ³•**: æ”¯æŒç®€å• HTTP è¯·æ±‚ã€Scrapy æ¡†æ¶å’Œæµè§ˆå™¨è‡ªåŠ¨åŒ–
- **æ™ºèƒ½æ–¹æ³•é€‰æ‹©**: è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„çˆ¬å–æ–¹æ³•
- **å¹¶å‘å¤„ç†**: æ”¯æŒå¤šä¸ª URL çš„å¹¶å‘çˆ¬å–
- **é…ç½®åŒ–æå–**: çµæ´»çš„æ•°æ®æå–é…ç½®ç³»ç»Ÿ

### é«˜çº§åŠŸèƒ½

- **ååçˆ¬è™«**: ä½¿ç”¨ undetected-chromedriver å’Œ Playwright çš„éšèº«æŠ€æœ¯
- **è¡¨å•å¤„ç†**: è‡ªåŠ¨å¡«å†™å’Œæäº¤å„ç§ç±»å‹çš„è¡¨å•
- **JavaScript æ”¯æŒ**: å®Œæ•´çš„æµè§ˆå™¨æ¸²æŸ“æ”¯æŒ
- **æ™ºèƒ½é‡è¯•**: æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
- **ç»“æœç¼“å­˜**: å†…å­˜ç¼“å­˜æå‡æ€§èƒ½

### ä¼ä¸šçº§ç‰¹æ€§

- **é”™è¯¯å¤„ç†**: å®Œå–„çš„é”™è¯¯åˆ†ç±»å’Œå¤„ç†
- **æ€§èƒ½ç›‘æ§**: è¯¦ç»†çš„è¯·æ±‚æŒ‡æ ‡å’Œç»Ÿè®¡
- **é€Ÿç‡é™åˆ¶**: é˜²æ­¢æœåŠ¡å™¨è¿‡è½½
- **ä»£ç†æ”¯æŒ**: æ”¯æŒ HTTP ä»£ç†é…ç½®
- **éšæœº UA**: é˜²æ£€æµ‹çš„ç”¨æˆ·ä»£ç†è½®æ¢

## ğŸ“‹ é¡¹ç›®ç°çŠ¶

### é¡¹ç›®ç»“æ„

```
data-extractor/
â”œâ”€â”€ extractor/                          # æ ¸å¿ƒå¼•æ“æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py                     # åŒ…åˆå§‹åŒ– (v0.1.4)
â”‚   â”œâ”€â”€ server.py                       # FastMCP æœåŠ¡å™¨ä¸ 14 ä¸ª MCP å·¥å…·
â”‚   â”œâ”€â”€ scraper.py                      # WebScraper æ ¸å¿ƒæŠ“å–å¼•æ“
â”‚   â”œâ”€â”€ advanced_features.py            # åæ£€æµ‹ä¸è¡¨å•è‡ªåŠ¨åŒ–
â”‚   â”œâ”€â”€ markdown_converter.py           # Markdown è½¬æ¢å¼•æ“ (8ç§æ ¼å¼åŒ–é€‰é¡¹)
â”‚   â”œâ”€â”€ pdf_processor.py               # PDF å¤„ç†å¼•æ“ (PyMuPDF/PyPDF2åŒå¼•æ“)
â”‚   â”œâ”€â”€ config.py                       # é…ç½®ç®¡ç† (DataExtractorSettings)
â”‚   â””â”€â”€ utils.py                        # ä¼ä¸šçº§å·¥å…·é›† (é™æµã€é‡è¯•ã€ç¼“å­˜ç­‰)
â”œâ”€â”€ examples/                           # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ basic_usage.py                  # åŸºç¡€ç”¨æ³•ç¤ºä¾‹
â”‚   â””â”€â”€ extraction_configs.py           # æ•°æ®æå–é…ç½®ç¤ºä¾‹
â”œâ”€â”€ tests/                              # å®Œæ•´æµ‹è¯•ä½“ç³» (191ä¸ªæµ‹è¯•)
â”‚   â”œâ”€â”€ unit/                           # å•å…ƒæµ‹è¯• (98ä¸ªæµ‹è¯•)
â”‚   â”‚   â”œâ”€â”€ test_scraper.py              # WebScraper æ ¸å¿ƒå¼•æ“æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_advanced_features.py    # é«˜çº§åŠŸèƒ½æµ‹è¯• (åæ£€æµ‹ã€è¡¨å•å¤„ç†)
â”‚   â”‚   â”œâ”€â”€ test_utils.py                # å·¥å…·ç±»æµ‹è¯• (é™æµã€é‡è¯•ã€ç¼“å­˜ç­‰)
â”‚   â”‚   â”œâ”€â”€ test_markdown_converter.py   # MarkdownConverter æµ‹è¯•
â”‚   â”‚   â””â”€â”€ test_pdf_processor.py        # PDF å¤„ç†å¼•æ“æµ‹è¯•
â”‚   â”œâ”€â”€ integration/                     # é›†æˆæµ‹è¯• (93ä¸ªæµ‹è¯•)
â”‚   â”‚   â”œâ”€â”€ test_mcp_tools.py            # 12ä¸ªMCPå·¥å…·é›†æˆæµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_comprehensive_integration.py # ç»¼åˆé›†æˆæµ‹è¯• (ç«¯åˆ°ç«¯ã€æ€§èƒ½ã€å®é™…åœºæ™¯)
â”‚   â”‚   â”œâ”€â”€ test_pdf_integration.py      # PDF å·¥å…·å®é™…æ‰§è¡ŒéªŒè¯ (13é¡¹)
â”‚   â”‚   â”œâ”€â”€ test_cross_tool_integration.py # è·¨å·¥å…·åä½œæµç¨‹éªŒè¯ (9é¡¹)
â”‚   â”‚   â””â”€â”€ test_end_to_end_integration.py # ç«¯åˆ°ç«¯ç°å®åœºæ™¯æµ‹è¯• (34é¡¹)
â”‚   â””â”€â”€ conftest.py                      # pytest é…ç½®å’Œå…±äº« fixtures
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup.sh                        # å¿«é€Ÿå®‰è£…è„šæœ¬
â”œâ”€â”€ .claude/                            # Claude Code é…ç½®
â”‚   â””â”€â”€ settings.local.json             # IDE æœ¬åœ°è®¾ç½®
â”œâ”€â”€ .vscode/                            # VS Code é…ç½®
â”‚   â””â”€â”€ settings.json                   # ç¼–è¾‘å™¨è®¾ç½®
â”œâ”€â”€ TESTING.md                          # æµ‹è¯•æ–‡æ¡£ (30KB)
â”œâ”€â”€ TEST_RESULTS.md                     # æµ‹è¯•æ‰§è¡ŒæŠ¥å‘Š (9KB)
â”œâ”€â”€ CHANGELOG.md                        # ç‰ˆæœ¬å˜æ›´æ—¥å¿— (17KB)
â”œâ”€â”€ CLAUDE.md                           # Claude Code é¡¹ç›®æŒ‡å¯¼
â”œâ”€â”€ .prompts.md                         # é¡¹ç›®é‡Œç¨‹ç¢‘å’Œä»»åŠ¡æ¸…å•
â”œâ”€â”€ .env.example                        # ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹
â”œâ”€â”€ .mcp.json                           # MCP æœåŠ¡å™¨é…ç½®
â”œâ”€â”€ .gitignore                          # Git å¿½ç•¥è§„åˆ™
â”œâ”€â”€ mypy.ini                            # ç±»å‹æ£€æŸ¥é…ç½®
â”œâ”€â”€ pyproject.toml                      # é¡¹ç›®é…ç½®å’Œä¾èµ–ç®¡ç†
â””â”€â”€ uv.lock                             # ä¾èµ–é”å®šæ–‡ä»¶ (311KB)
```

### å·²å®Œæˆçš„é‡Œç¨‹ç¢‘ âœ…

- âœ… **v0.1.4 ç¨³å®šç‰ˆå‘å¸ƒ**: åŸºäº Scrapy + FastMCP æ„å»ºçš„ä¼ä¸šçº§ç½‘é¡µæŠ“å– MCP Server
- âœ… **å®Œæ•´æµ‹è¯•ä½“ç³»**: 219 ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œé€šè¿‡ç‡ 98.6%+ï¼ŒåŒ…å«å•å…ƒæµ‹è¯•å’Œå¼ºåŒ–é›†æˆæµ‹è¯•
- âœ… **é›†æˆæµ‹è¯•å¼ºåŒ–**: æ–°å¢ PDF å·¥å…·å®é™…æ‰§è¡ŒéªŒè¯ã€è·¨å·¥å…·åä½œæµç¨‹ã€ç«¯åˆ°ç«¯ç°å®åœºæ™¯æµ‹è¯•
- âœ… **ä»£ç è´¨é‡ä¼˜åŒ–**: ç±»å‹æ³¨è§£å®Œå–„ï¼Œä» black è¿ç§»åˆ° ruff æ ¼å¼åŒ–
- âœ… **é…ç½®ç»Ÿä¸€**: é¡¹ç›®åç§°ä» scrapy-mcp æ›´åä¸º data-extractorï¼Œé…ç½®å‰ç¼€ç»Ÿä¸€
- âœ… **æ–‡æ¡£å®Œå–„**: READMEã€CHANGELOGã€TESTING æ–‡æ¡£ä½“ç³»å»ºç«‹

### å½“å‰çŠ¶æ€ ğŸ“Š

- **ç‰ˆæœ¬**: v0.1.4 âœ¨
- **åŠŸèƒ½çŠ¶æ€**: 14 ä¸ª MCP å·¥å…· + Markdown è½¬æ¢åŠŸèƒ½å®Œæ•´ + PDF è½¬æ¢åŠŸèƒ½å®Œæ•´
- **æµ‹è¯•è¦†ç›–ç‡**: 98.6%+ (219 ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œå¼ºåŒ–é›†æˆæµ‹è¯•è¦†ç›–)
- **è´¨é‡è¯„çº§**: A+ (ç”Ÿäº§å°±ç»ªæ ‡å‡†)
- **ä»£ç æ ¼å¼åŒ–**: ruff
- **åŒ…ç®¡ç†**: uv
- **Python è¦æ±‚**: 3.12+
- **éƒ¨ç½²çŠ¶æ€**: æ”¯æŒæœ¬åœ°å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### v0.1.3 (2025-09-06)

- **Markdown è½¬æ¢åŠŸèƒ½**: æ–°å¢ 2 ä¸ª MCP å·¥å…·ï¼ŒåŒ…å«é¡µé¢è½¬ Markdown å’Œæ‰¹é‡è½¬æ¢åŠŸèƒ½
- **é«˜çº§æ ¼å¼åŒ–èƒ½åŠ›**: 8 ç§å¯é…ç½®æ ¼å¼åŒ–é€‰é¡¹ï¼ŒåŒ…æ‹¬è¡¨æ ¼å¯¹é½ã€ä»£ç è¯­è¨€æ£€æµ‹ã€æ™ºèƒ½æ’ç‰ˆ
- **å®Œæ•´æµ‹è¯•ä½“ç³»**: 162 ä¸ªæµ‹è¯•ç”¨ä¾‹ (131 ä¸ªå•å…ƒæµ‹è¯• + 31 ä¸ªé›†æˆæµ‹è¯•)ï¼Œé€šè¿‡ç‡ 99.4%
- **ç»¼åˆé›†æˆæµ‹è¯•**: ç«¯åˆ°ç«¯åŠŸèƒ½æµ‹è¯•ã€æ€§èƒ½è´Ÿè½½æµ‹è¯•ã€é”™è¯¯æ¢å¤éŸ§æ€§æµ‹è¯•ã€ç³»ç»Ÿå¥åº·è¯Šæ–­
- **æµ‹è¯•æ–‡æ¡£å®Œå–„**: è¯¦ç»†çš„ TESTING.md (åŒ…å«æµ‹è¯•æ¶æ„ã€æ‰§è¡ŒæŒ‡å—ã€æ•…éšœæ’é™¤)
- **è´¨é‡ä¿éšœ**: A+ è¯„çº§ï¼Œç”Ÿäº§å°±ç»ªæ ‡å‡†ï¼Œpytest å¼‚æ­¥æµ‹è¯•ã€Mock ç­–ç•¥ã€æ€§èƒ½åŸºå‡†

### v0.1.2 (2025-09-06)

- **æµ‹è¯•æ¡†æ¶å»ºè®¾**: å»ºç«‹å®Œæ•´çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•ä½“ç³»ï¼Œ19 ä¸ªåŸºç¡€æµ‹è¯•å…¨éƒ¨é€šè¿‡
- **æµ‹è¯•æ–‡æ¡£**: æ–°å¢ 67KB è¯¦ç»†æµ‹è¯•æ–‡æ¡£å’Œæ‰§è¡ŒæŠ¥å‘Š
- **è´¨é‡ä¿éšœ**: pytest å¼‚æ­¥æµ‹è¯•æ”¯æŒï¼ŒMock ç­–ç•¥å’Œæ€§èƒ½ä¼˜åŒ–

### v0.1.1 (2025-09-05)

- **æ ¸å¿ƒé‡æ„**: åŒ…åä» `scrapy_mcp` é‡æ„ä¸º `extractor`ï¼Œæå‡é¡¹ç›®ç»“æ„æ¸…æ™°åº¦
- **å‘½ä»¤æ›´æ–°**: é¡¹ç›®å…¥å£å‘½ä»¤ç»Ÿä¸€ä¸º `data-extractor`
- **æ–‡æ¡£å®Œå–„**: æ›´æ–°æ‰€æœ‰é…ç½®ç¤ºä¾‹å’Œå®‰è£…è¯´æ˜

### v0.1.0 (2025-08-26)

- **åˆå§‹å‘å¸ƒ**: å®Œæ•´çš„ç½‘é¡µçˆ¬å– MCP Server å®ç°
- **æ ¸å¿ƒåŠŸèƒ½**: 10 ä¸ªä¸“ä¸šçˆ¬å–å·¥å…·ï¼Œæ”¯æŒå¤šç§åœºæ™¯
- **ä¼ä¸šç‰¹æ€§**: é€Ÿç‡é™åˆ¶ã€æ™ºèƒ½é‡è¯•ã€ç¼“å­˜æœºåˆ¶
- **æŠ€æœ¯æ ˆ**: è¿ç§»è‡³ uv åŒ…ç®¡ç†ï¼Œå¢å¼ºå¼€å‘ä½“éªŒ

## ğŸš¦ å¿«é€Ÿå¼€å§‹

### ğŸ“¦ å®‰è£…

```bash
# ç¡®è®¤ Python ç‰ˆæœ¬ (éœ€è¦ 3.12+)
python --version

# å…‹éš†ä»“åº“
git clone https://github.com/ThreeFish-AI/data-extractor.git
cd data-extractor

# å¿«é€Ÿè®¾ç½®ï¼ˆæ¨èï¼‰
./scripts/setup.sh

# æˆ–æ‰‹åŠ¨å®‰è£…
# ä½¿ç”¨ uv å®‰è£…ä¾èµ–
uv sync

# å®‰è£…åŒ…æ‹¬å¼€å‘ä¾èµ–
uv sync --extra dev

# æˆ–è€…ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
pip install -e .

# æˆ–è€…ä½¿ç”¨å¼€å‘æ¨¡å¼
pip install -e ".[dev]"
```

### ğŸ”§ é…ç½®

åˆ›å»º `.env` æ–‡ä»¶æ¥è‡ªå®šä¹‰é…ç½®ï¼š

```bash
# æœåŠ¡å™¨è®¾ç½®
DATA_EXTRACTOR_SERVER_NAME=data-extractor
# DATA_EXTRACTOR_SERVER_VERSION=auto  # ç‰ˆæœ¬å·è‡ªåŠ¨ä» pyproject.toml è¯»å–ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®

# å¹¶å‘å’Œå»¶è¿Ÿè®¾ç½®
DATA_EXTRACTOR_CONCURRENT_REQUESTS=16
DATA_EXTRACTOR_DOWNLOAD_DELAY=1.0
DATA_EXTRACTOR_RANDOMIZE_DOWNLOAD_DELAY=true

# æµè§ˆå™¨è®¾ç½®
DATA_EXTRACTOR_ENABLE_JAVASCRIPT=false
DATA_EXTRACTOR_BROWSER_HEADLESS=true
DATA_EXTRACTOR_BROWSER_TIMEOUT=30

# åæ£€æµ‹è®¾ç½®
DATA_EXTRACTOR_USE_RANDOM_USER_AGENT=true
DATA_EXTRACTOR_USE_PROXY=false
DATA_EXTRACTOR_PROXY_URL=

# é‡è¯•è®¾ç½®
DATA_EXTRACTOR_MAX_RETRIES=3
DATA_EXTRACTOR_REQUEST_TIMEOUT=30
```

### å¯åŠ¨æœåŠ¡å™¨

```bash
# ä½¿ç”¨å‘½ä»¤è¡Œ
data-extractor

# ä½¿ç”¨ uv è¿è¡Œï¼ˆæ¨èï¼‰
uv run data-extractor

# æˆ–è€…ä½¿ç”¨Python
python -m extractor.server

# ä½¿ç”¨ uv è¿è¡Œ Python æ¨¡å—
uv run python -m extractor.server
```

### MCP Client é…ç½®

åœ¨æ‚¨çš„ MCP client (å¦‚ Claude Desktop) ä¸­æ·»åŠ æœåŠ¡å™¨é…ç½®ï¼š

#### æ–¹å¼ä¸€ï¼šç›´æ¥å‘½ä»¤æ–¹å¼

```json
{
  "mcpServers": {
    "data-extractor": {
      "command": "data-extractor",
      "args": []
    }
  }
}
```

#### æ–¹å¼äºŒï¼šé€šè¿‡ uv å¯åŠ¨ï¼ˆæ¨èï¼‰

```json
{
  "mcpServers": {
    "data-extractor": {
      "command": "uv",
      "args": ["run", "data-extractor"],
      "cwd": "/path/to/your/data-extractor"
    }
  }
}
```

#### æ–¹å¼ä¸‰ï¼šä» GitHub ä»“åº“ç›´æ¥å®‰è£…å’Œè¿è¡Œï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

```json
{
  "mcpServers": {
    "data-extractor": {
      "command": "uv",
      "args": [
        "run",
        "--with",
        "git+https://github.com/ThreeFish-AI/data-extractor.git@v0.1.4",
        "data-extractor"
      ]
    }
  }
}
```

#### æ–¹å¼å››ï¼šPython æ¨¡å—æ–¹å¼ï¼ˆæœ¬åœ°å¼€å‘ï¼‰

```json
{
  "mcpServers": {
    "data-extractor": {
      "command": "uv",
      "args": ["run", "python", "-m", "extractor.server"],
      "cwd": "/path/to/your/data-extractor"
    }
  }
}
```

**æ³¨æ„äº‹é¡¹ï¼š**

- å°† `cwd` è·¯å¾„æ›¿æ¢ä¸ºæ‚¨çš„é¡¹ç›®å®é™…è·¯å¾„
- GitHub ä»“åº“åœ°å€ï¼š`https://github.com/ThreeFish-AI/data-extractor.git`
- æ¨èä½¿ç”¨æ–¹å¼äºŒï¼ˆæœ¬åœ° uv å¯åŠ¨ï¼‰è¿›è¡Œå¼€å‘ï¼Œæ–¹å¼ä¸‰ï¼ˆGitHub ç›´æ¥å®‰è£…ï¼‰ç”¨äºç”Ÿäº§ç¯å¢ƒ
- å½“å‰æœ€æ–°ç¨³å®šç‰ˆæœ¬ï¼šv0.1.4

## ğŸ§ª æµ‹è¯•è¿è¡Œ

### å¿«é€Ÿæµ‹è¯•éªŒè¯

```bash
# å®‰è£…å¼€å‘ä¾èµ–
uv sync --extra dev

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
uv run pytest

# è¿è¡Œé›†æˆæµ‹è¯•
uv run pytest tests/integration/ -v

# è¿è¡Œå•å…ƒæµ‹è¯•
uv run pytest tests/unit/ -v
```

### æµ‹è¯•è¦†ç›–æƒ…å†µ

- **æ€»æµ‹è¯•æ•°**: 191 ä¸ªæµ‹è¯•ç”¨ä¾‹
- **é€šè¿‡ç‡**: 95%+ (å¼ºåŒ–é›†æˆæµ‹è¯•åçš„è¦†ç›–ç‡)
- **å•å…ƒæµ‹è¯•**: 98 ä¸ª - æ ¸å¿ƒç»„ä»¶åŠŸèƒ½æµ‹è¯•
- **é›†æˆæµ‹è¯•**: 93 ä¸ª - ç«¯åˆ°ç«¯åŠŸèƒ½å’Œç³»ç»Ÿå¥åº·æµ‹è¯•ï¼ŒåŒ…å«å¼ºåŒ–æµ‹è¯•

### æµ‹è¯•ç±»å‹

| æµ‹è¯•ç±»åˆ«            | æµ‹è¯•æ•°é‡ | è¦†ç›–èŒƒå›´                         |
| ------------------- | -------- | -------------------------------- |
| WebScraper æ ¸å¿ƒå¼•æ“ | 35+      | æ–¹æ³•é€‰æ‹©ã€æ•°æ®æå–ã€é”™è¯¯å¤„ç†     |
| é«˜çº§åŠŸèƒ½æµ‹è¯•        | 25+      | åæ£€æµ‹ã€è¡¨å•å¤„ç†ã€æµè§ˆå™¨è‡ªåŠ¨åŒ–   |
| å·¥å…·ç±»æµ‹è¯•          | 30+      | é™æµã€é‡è¯•ã€ç¼“å­˜ã€æŒ‡æ ‡æ”¶é›†       |
| Markdown è½¬æ¢å™¨     | 8        | HTML è½¬æ¢ã€æ ¼å¼åŒ–ã€æ‰¹é‡å¤„ç†      |
| MCP å·¥å…·é›†æˆ        | 37       | 14 ä¸ª MCP å·¥å…·æ³¨å†Œå’ŒåŠŸèƒ½éªŒè¯     |
| PDF å·¥å…·å®é™…è°ƒç”¨    | 13       | PDF å¤„ç†å·¥å…·çš„å®é™… MCP æ‰§è¡ŒéªŒè¯  |
| è·¨å·¥å…·åä½œæµ‹è¯•      | 9        | å¤šå·¥å…·å·¥ä½œæµã€å‚æ•°ä¼ é€’ã€é”™è¯¯ä¼ æ’­ |
| ç«¯åˆ°ç«¯ç°å®åœºæ™¯      | 34       | å®Œæ•´å¤„ç†ç®¡é“ã€æ€§èƒ½åŸºå‡†ã€æ•°æ®å®Œæ•´ |

### è¯¦ç»†æµ‹è¯•æ–‡æ¡£

æŸ¥çœ‹ [TESTING.md](./TESTING.md) äº†è§£å®Œæ•´çš„æµ‹è¯•æ¶æ„ã€æ‰§è¡ŒæŒ‡å—å’Œæ•…éšœæ’é™¤æ–¹æ³•ã€‚

## ğŸ› ï¸ å·¥å…·è¯¦æƒ…

### 1. scrape_webpage

åŸºç¡€ç½‘é¡µçˆ¬å–å·¥å…·ï¼Œæ”¯æŒå¤šç§æ–¹æ³•å’Œè‡ªå®šä¹‰æå–è§„åˆ™ã€‚

**å‚æ•°:**

- `url`: è¦çˆ¬å–çš„ URL
- `method`: çˆ¬å–æ–¹æ³• (auto/simple/scrapy/selenium)
- `extract_config`: æ•°æ®æå–é…ç½® (å¯é€‰)
- `wait_for_element`: ç­‰å¾…çš„ CSS é€‰æ‹©å™¨ (Selenium ä¸“ç”¨)

**ç¤ºä¾‹:**

```json
{
  "url": "https://example.com",
  "method": "auto",
  "extract_config": {
    "title": "h1",
    "content": {
      "selector": ".content p",
      "multiple": true,
      "attr": "text"
    }
  }
}
```

### 2. scrape_multiple_webpages

å¹¶å‘çˆ¬å–å¤šä¸ªç½‘é¡µã€‚

**ç¤ºä¾‹:**

```json
{
  "urls": ["https://example1.com", "https://example2.com"],
  "method": "simple",
  "extract_config": {
    "title": "h1",
    "links": "a"
  }
}
```

### 3. scrape_with_stealth

ä½¿ç”¨é«˜çº§åæ£€æµ‹æŠ€æœ¯çˆ¬å–ç½‘é¡µã€‚

**å‚æ•°:**

- `url`: ç›®æ ‡ URL
- `method`: éšèº«æ–¹æ³• (selenium/playwright)
- `extract_config`: æå–é…ç½®
- `wait_for_element`: ç­‰å¾…å…ƒç´ 
- `scroll_page`: æ˜¯å¦æ»šåŠ¨é¡µé¢åŠ è½½åŠ¨æ€å†…å®¹

**ç¤ºä¾‹:**

```json
{
  "url": "https://protected-site.com",
  "method": "playwright",
  "scroll_page": true,
  "wait_for_element": ".dynamic-content"
}
```

### 4. fill_and_submit_form

è¡¨å•å¡«å†™å’Œæäº¤ã€‚

**å‚æ•°:**

- `url`: åŒ…å«è¡¨å•çš„é¡µé¢ URL
- `form_data`: è¡¨å•å­—æ®µæ•°æ® (é€‰æ‹©å™¨:å€¼ å¯¹)
- `submit`: æ˜¯å¦æäº¤è¡¨å•
- `submit_button_selector`: æäº¤æŒ‰é’®é€‰æ‹©å™¨
- `method`: æ–¹æ³• (selenium/playwright)

**ç¤ºä¾‹:**

```json
{
  "url": "https://example.com/contact",
  "form_data": {
    "input[name='name']": "John Doe",
    "input[name='email']": "john@example.com",
    "textarea[name='message']": "Hello world"
  },
  "submit": true,
  "method": "selenium"
}
```

### 5. extract_links

ä¸“é—¨çš„é“¾æ¥æå–å·¥å…·ã€‚

**å‚æ•°:**

- `url`: ç›®æ ‡ URL
- `filter_domains`: åªåŒ…å«è¿™äº›åŸŸåçš„é“¾æ¥
- `exclude_domains`: æ’é™¤è¿™äº›åŸŸåçš„é“¾æ¥
- `internal_only`: åªæå–å†…éƒ¨é“¾æ¥

**ç¤ºä¾‹:**

```json
{
  "url": "https://example.com",
  "internal_only": true
}
```

### 6. extract_structured_data

è‡ªåŠ¨æå–ç»“æ„åŒ–æ•°æ® (è”ç³»ä¿¡æ¯ã€ç¤¾äº¤åª’ä½“é“¾æ¥ç­‰)ã€‚

**å‚æ•°:**

- `url`: ç›®æ ‡ URL
- `data_type`: æ•°æ®ç±»å‹ (all/contact/social/content)

**ç¤ºä¾‹:**

```json
{
  "url": "https://company.com",
  "data_type": "contact"
}
```

### 7. get_page_info

å¿«é€Ÿè·å–é¡µé¢åŸºç¡€ä¿¡æ¯ã€‚

**ç¤ºä¾‹:**

```json
{
  "url": "https://example.com"
}
```

### 8. check_robots_txt

æ£€æŸ¥ç½‘ç«™çš„ robots.txt æ–‡ä»¶ã€‚

### 9. get_server_metrics

è·å–æœåŠ¡å™¨æ€§èƒ½æŒ‡æ ‡å’Œç»Ÿè®¡ä¿¡æ¯ã€‚

### 10. clear_cache

æ¸…é™¤ç¼“å­˜çš„çˆ¬å–ç»“æœã€‚

### 11. convert_webpage_to_markdown

å°†ç½‘é¡µå†…å®¹æŠ“å–å¹¶è½¬æ¢ä¸º Markdown æ ¼å¼ï¼Œé€‚ç”¨äºæ–‡æ¡£å¤„ç†ã€å†…å®¹åˆ†æå’Œå­˜å‚¨ã€‚

**å‚æ•°:**

- `url`: è¦æŠ“å–å’Œè½¬æ¢çš„ URL
- `method`: æŠ“å–æ–¹æ³• (auto/simple/scrapy/seleniumï¼Œé»˜è®¤ auto)
- `extract_main_content`: æ˜¯å¦ä»…æå–ä¸»è¦å†…å®¹åŒºåŸŸ (é»˜è®¤ true)
- `include_metadata`: æ˜¯å¦åŒ…å«é¡µé¢å…ƒæ•°æ® (é»˜è®¤ true)
- `custom_options`: è‡ªå®šä¹‰ Markdown è½¬æ¢é€‰é¡¹ (å¯é€‰)
- `wait_for_element`: ç­‰å¾…çš„ CSS é€‰æ‹©å™¨ (Selenium ä¸“ç”¨)
- `formatting_options`: é«˜çº§æ ¼å¼åŒ–é€‰é¡¹ï¼ŒåŒ…å«ä»¥ä¸‹é…ç½®:
  - `format_tables`: è¡¨æ ¼å¯¹é½æ ¼å¼åŒ– (é»˜è®¤ true)
  - `detect_code_language`: è‡ªåŠ¨æ£€æµ‹ä»£ç è¯­è¨€ (é»˜è®¤ true)
  - `format_quotes`: å¼•ç”¨å—æ ¼å¼åŒ– (é»˜è®¤ true)
  - `enhance_images`: å›¾ç‰‡æè¿°å¢å¼º (é»˜è®¤ true)
  - `optimize_links`: é“¾æ¥æ ¼å¼ä¼˜åŒ– (é»˜è®¤ true)
  - `format_lists`: åˆ—è¡¨æ ¼å¼åŒ– (é»˜è®¤ true)
  - `format_headings`: æ ‡é¢˜æ ¼å¼åŒ–å’Œé—´è· (é»˜è®¤ true)
  - `apply_typography`: æ’ç‰ˆä¼˜åŒ– (æ™ºèƒ½å¼•å·ã€ç ´æŠ˜å·ç­‰ï¼Œé»˜è®¤ true)

**åŠŸèƒ½ç‰¹æ€§:**

- **æ™ºèƒ½å†…å®¹æå–**: è‡ªåŠ¨è¯†åˆ«å¹¶æå–ç½‘é¡µä¸»è¦å†…å®¹åŒºåŸŸ
- **æ¸…ç†å¤„ç†**: ç§»é™¤å¹¿å‘Šã€å¯¼èˆªæ ã€ä¾§è¾¹æ ç­‰æ— å…³å†…å®¹
- **URL è½¬æ¢**: å°†ç›¸å¯¹ URL è½¬æ¢ä¸ºç»å¯¹ URL
- **æ ¼å¼ä¼˜åŒ–**: æ¸…ç†å¤šä½™ç©ºç™½è¡Œï¼Œä¼˜åŒ– Markdown æ ¼å¼
- **å…ƒæ•°æ®ä¸°å¯Œ**: åŒ…å«æ ‡é¢˜ã€æè¿°ã€å­—æ•°ç»Ÿè®¡ç­‰ä¿¡æ¯
- **é«˜çº§æ ¼å¼åŒ–**: æä¾› 8 ç§å¯é…ç½®çš„æ ¼å¼åŒ–é€‰é¡¹
  - è¡¨æ ¼è‡ªåŠ¨å¯¹é½å’Œæ ¼å¼åŒ–
  - ä»£ç å—è¯­è¨€è‡ªåŠ¨æ£€æµ‹ (æ”¯æŒ Pythonã€JavaScriptã€HTMLã€SQL ç­‰)
  - å¼•ç”¨å—æ ¼å¼ä¼˜åŒ–
  - å›¾ç‰‡æè¿°è‡ªåŠ¨ç”Ÿæˆå’Œå¢å¼º
  - é“¾æ¥æ ¼å¼ä¼˜åŒ–å’Œå»é‡
  - åˆ—è¡¨æ ¼å¼ç»Ÿä¸€åŒ–
  - æ ‡é¢˜å±‚çº§å’Œé—´è·ä¼˜åŒ–
  - æ’ç‰ˆå¢å¼º (æ™ºèƒ½å¼•å·ã€em ç ´æŠ˜å·ã€ç©ºæ ¼æ¸…ç†)

**ç¤ºä¾‹:**

```json
{
  "url": "https://example.com/article",
  "method": "auto",
  "extract_main_content": true,
  "include_metadata": true,
  "custom_options": {
    "heading_style": "ATX",
    "bullets": "-",
    "wrap": false
  },
  "formatting_options": {
    "format_tables": true,
    "detect_code_language": true,
    "enhance_images": true,
    "apply_typography": false
  }
}
```

**è¿”å›ç¤ºä¾‹:**

```json
{
  "success": true,
  "data": {
    "url": "https://example.com/article",
    "markdown": "# Article Title\n\nThis is the article content...",
    "metadata": {
      "title": "Article Title",
      "meta_description": "Article description",
      "word_count": 500,
      "character_count": 3000,
      "domain": "example.com"
    }
  }
}
```

### 12. batch_convert_webpages_to_markdown

æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µå¹¶è½¬æ¢ä¸º Markdown æ ¼å¼ï¼Œæ”¯æŒå¹¶å‘å¤„ç†æå‡æ•ˆç‡ã€‚

**å‚æ•°:**

- `urls`: è¦æŠ“å–å’Œè½¬æ¢çš„ URL åˆ—è¡¨
- `method`: æŠ“å–æ–¹æ³• (auto/simple/scrapy/seleniumï¼Œé»˜è®¤ auto)
- `extract_main_content`: æ˜¯å¦ä»…æå–ä¸»è¦å†…å®¹åŒºåŸŸ (é»˜è®¤ true)
- `include_metadata`: æ˜¯å¦åŒ…å«é¡µé¢å…ƒæ•°æ® (é»˜è®¤ true)
- `custom_options`: è‡ªå®šä¹‰ Markdown è½¬æ¢é€‰é¡¹ (å¯é€‰)
- `formatting_options`: é«˜çº§æ ¼å¼åŒ–é€‰é¡¹ (ä¸å•é¡µè½¬æ¢ç›¸åŒé…ç½®)

**åŠŸèƒ½ç‰¹æ€§:**

- **å¹¶å‘å¤„ç†**: åŒæ—¶å¤„ç†å¤šä¸ª URL æå‡æ•ˆç‡
- **ä¸€è‡´æ ¼å¼**: æ‰€æœ‰é¡µé¢ä½¿ç”¨ç›¸åŒçš„è½¬æ¢é…ç½®
- **è¯¦ç»†ç»Ÿè®¡**: æä¾›æˆåŠŸ/å¤±è´¥ç»Ÿè®¡å’Œæ±‡æ€»ä¿¡æ¯
- **é”™è¯¯å¤„ç†**: å•ä¸ªé¡µé¢å¤±è´¥ä¸å½±å“å…¶ä»–é¡µé¢å¤„ç†
- **æ‰¹é‡ä¼˜åŒ–**: é’ˆå¯¹å¤§é‡é¡µé¢ä¼˜åŒ–çš„æ€§èƒ½é…ç½®

**ç¤ºä¾‹:**

```json
{
  "urls": [
    "https://example.com/article1",
    "https://example.com/article2",
    "https://example.com/article3"
  ],
  "method": "auto",
  "extract_main_content": true,
  "include_metadata": true
}
```

**è¿”å›ç¤ºä¾‹:**

```json
{
  "success": true,
  "data": {
    "results": [
      {
        "success": true,
        "url": "https://example.com/article1",
        "markdown": "# Article 1\n\nContent...",
        "metadata": {...}
      },
      {
        "success": true,
        "url": "https://example.com/article2",
        "markdown": "# Article 2\n\nContent...",
        "metadata": {...}
      }
    ],
    "summary": {
      "total": 3,
      "successful": 2,
      "failed": 1,
      "success_rate": 0.67
    }
  }
}
```

### 13. convert_pdf_to_markdown

å°† PDF æ–‡æ¡£è½¬æ¢ä¸º Markdown æ ¼å¼ï¼Œæ”¯æŒ URL å’Œæœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œé€‚ç”¨äºæ–‡æ¡£å¤„ç†ã€å†…å®¹åˆ†æå’ŒçŸ¥è¯†ç®¡ç†ã€‚

**å‚æ•°:**

- `pdf_source`: PDF URL æˆ–æœ¬åœ°æ–‡ä»¶è·¯å¾„
- `method`: æå–æ–¹æ³• (auto/pymupdf/pypdf2ï¼Œé»˜è®¤ auto)
- `include_metadata`: æ˜¯å¦åŒ…å« PDF å…ƒæ•°æ® (é»˜è®¤ true)
- `page_range`: é¡µé¢èŒƒå›´ [start, end] ç”¨äºéƒ¨åˆ†æå– (å¯é€‰)
- `output_format`: è¾“å‡ºæ ¼å¼ (markdown/textï¼Œé»˜è®¤ markdown)

**åŠŸèƒ½ç‰¹æ€§:**

- **å¤šæºæ”¯æŒ**: æ”¯æŒ PDF URL å’Œæœ¬åœ°æ–‡ä»¶è·¯å¾„
- **å¤šå¼•æ“æ”¯æŒ**: PyMuPDF (fitz) å’Œ PyPDF2 åŒå¼•æ“è‡ªåŠ¨é€‰æ‹©
- **éƒ¨åˆ†æå–**: æ”¯æŒæŒ‡å®šé¡µé¢èŒƒå›´çš„éƒ¨åˆ†æå–
- **å…ƒæ•°æ®æå–**: åŒ…å«æ ‡é¢˜ã€ä½œè€…ã€åˆ›å»ºæ—¥æœŸç­‰å®Œæ•´å…ƒæ•°æ®
- **æ™ºèƒ½è½¬æ¢**: è‡ªåŠ¨æ£€æµ‹æ ‡é¢˜å±‚çº§å’Œæ ¼å¼åŒ–
- **é”™è¯¯æ¢å¤**: å¼•æ“å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢å¤‡ç”¨æ–¹æ³•

**ç¤ºä¾‹:**

```json
{
  "pdf_source": "https://example.com/document.pdf",
  "method": "auto",
  "include_metadata": true,
  "page_range": [0, 10],
  "output_format": "markdown"
}
```

**è¿”å›ç¤ºä¾‹:**

```json
{
  "success": true,
  "data": {
    "text": "åŸå§‹æå–çš„æ–‡æœ¬å†…å®¹",
    "markdown": "# æ–‡æ¡£æ ‡é¢˜\n\nè½¬æ¢åçš„ Markdown å†…å®¹...",
    "metadata": {
      "title": "æ–‡æ¡£æ ‡é¢˜",
      "author": "ä½œè€…å§“å",
      "total_pages": 50,
      "pages_processed": 10,
      "file_size_bytes": 1024000
    },
    "source": "https://example.com/document.pdf",
    "method_used": "pymupdf",
    "word_count": 2500,
    "character_count": 15000
  }
}
```

### 14. batch_convert_pdfs_to_markdown

æ‰¹é‡è½¬æ¢å¤šä¸ª PDF æ–‡æ¡£ä¸º Markdown æ ¼å¼ï¼Œæ”¯æŒå¹¶å‘å¤„ç†æå‡æ•ˆç‡ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ–‡æ¡£å¤„ç†ã€‚

**å‚æ•°:**

- `pdf_sources`: PDF URL æˆ–æœ¬åœ°æ–‡ä»¶è·¯å¾„åˆ—è¡¨
- `method`: æå–æ–¹æ³• (auto/pymupdf/pypdf2ï¼Œé»˜è®¤ auto)
- `include_metadata`: æ˜¯å¦åŒ…å« PDF å…ƒæ•°æ® (é»˜è®¤ true)
- `page_range`: é¡µé¢èŒƒå›´ [start, end] åº”ç”¨äºæ‰€æœ‰ PDF (å¯é€‰)
- `output_format`: è¾“å‡ºæ ¼å¼ (markdown/textï¼Œé»˜è®¤ markdown)

**åŠŸèƒ½ç‰¹æ€§:**

- **å¹¶å‘å¤„ç†**: åŒæ—¶å¤„ç†å¤šä¸ª PDF æ–‡æ¡£æå‡æ•ˆç‡
- **ä¸€è‡´é…ç½®**: æ‰€æœ‰ PDF ä½¿ç”¨ç›¸åŒçš„è½¬æ¢è®¾ç½®
- **è¯¦ç»†ç»Ÿè®¡**: æä¾›æˆåŠŸ/å¤±è´¥ç»Ÿè®¡å’Œæ±‡æ€»ä¿¡æ¯
- **é”™è¯¯å®¹é”™**: å•ä¸ª PDF å¤±è´¥ä¸å½±å“å…¶ä»–æ–‡æ¡£å¤„ç†
- **æ‰¹é‡ä¼˜åŒ–**: é’ˆå¯¹å¤§é‡æ–‡æ¡£ä¼˜åŒ–çš„å†…å­˜å’Œæ€§èƒ½é…ç½®

**ç¤ºä¾‹:**

```json
{
  "pdf_sources": [
    "https://example.com/doc1.pdf",
    "/local/path/doc2.pdf",
    "https://example.com/doc3.pdf"
  ],
  "method": "auto",
  "include_metadata": true,
  "output_format": "markdown"
}
```

**è¿”å›ç¤ºä¾‹:**

```json
{
  "success": true,
  "data": {
    "results": [
      {
        "success": true,
        "source": "https://example.com/doc1.pdf",
        "text": "åŸå§‹æ–‡æœ¬å†…å®¹",
        "markdown": "# æ–‡æ¡£1æ ‡é¢˜\n\nå†…å®¹...",
        "metadata": {...},
        "word_count": 1500
      },
      {
        "success": true,
        "source": "/local/path/doc2.pdf",
        "text": "åŸå§‹æ–‡æœ¬å†…å®¹",
        "markdown": "# æ–‡æ¡£2æ ‡é¢˜\n\nå†…å®¹...",
        "metadata": {...},
        "word_count": 2000
      }
    ],
    "summary": {
      "total_pdfs": 3,
      "successful": 2,
      "failed": 1,
      "total_pages_processed": 45,
      "total_words_extracted": 3500,
      "method_used": "auto",
      "output_format": "markdown"
    }
  }
}
```

## ğŸ“– æ•°æ®æå–é…ç½®

### ç®€å•é€‰æ‹©å™¨

```json
{
  "title": "h1",
  "links": "a"
}
```

### é«˜çº§é…ç½®

```json
{
  "products": {
    "selector": ".product",
    "multiple": true,
    "attr": "text"
  },
  "prices": {
    "selector": ".price",
    "multiple": true,
    "attr": "data-price"
  },
  "description": {
    "selector": ".description",
    "multiple": false,
    "attr": "text"
  }
}
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### Data Extractor æ ¸å¿ƒå¼•æ“å±‚

Data Extractor æ ¸å¿ƒå¼•æ“é‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œæä¾›ç¨³å®šå¯é çš„ç½‘é¡µæŠ“å–èƒ½åŠ›ï¼š

#### 1. WebScraper ä¸»æ§åˆ¶å™¨ (`extractor/scraper.py`)

**è®¾è®¡ç†å¿µ**: ç»Ÿä¸€æ¥å£ï¼Œæ™ºèƒ½æ–¹æ³•é€‰æ‹©

```python
class WebScraper:
    """ä¸»æ§åˆ¶å™¨ï¼Œåè°ƒå„ç§æŠ“å–æ–¹æ³•"""

    def __init__(self):
        self.scrapy_wrapper = ScrapyWrapper()      # Scrapy æ¡†æ¶å°è£…
        self.selenium_scraper = SeleniumScraper()  # æµè§ˆå™¨è‡ªåŠ¨åŒ–
        self.simple_scraper = SimpleScraper()      # HTTP è¯·æ±‚

    async def scrape_url(self, url: str, method: str = "auto",
                         extract_config: Optional[Dict] = None) -> Dict:
        """æ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„æŠ“å–æ–¹æ³•"""
```

**æ ¸å¿ƒç‰¹æ€§**:

- **æ–¹æ³•è‡ªé€‰**: æ ¹æ® JavaScript éœ€æ±‚å’Œåæ£€æµ‹è¦æ±‚è‡ªåŠ¨é€‰æ‹© simple/scrapy/selenium
- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æŠ“å–æ–¹æ³•é€šè¿‡ç»Ÿä¸€çš„ `scrape_url()` æ¥å£è°ƒç”¨
- **å¹¶å‘æ”¯æŒ**: `scrape_multiple_urls()` å®ç°é«˜æ•ˆæ‰¹é‡å¤„ç†
- **é…ç½®åŒ–æå–**: æ”¯æŒ CSS é€‰æ‹©å™¨ã€å±æ€§æå–ã€å¤šå…ƒç´ æ‰¹é‡è·å–

#### 2. é«˜çº§åŠŸèƒ½æ¨¡å— (`extractor/advanced_features.py`)

**AntiDetectionScraper åæ£€æµ‹å¼•æ“**:

```python
class AntiDetectionScraper:
    """åæ£€æµ‹ä¸“ç”¨æŠ“å–å™¨"""

    async def scrape_with_stealth(self, url: str, method: str = "selenium"):
        """ä½¿ç”¨åæ£€æµ‹æŠ€æœ¯æŠ“å–"""
        # æ”¯æŒ undetected-chromedriver å’Œ Playwright åŒå¼•æ“
        # è‡ªåŠ¨æ³¨å…¥éšèº«è„šæœ¬ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
```

**FormHandler è¡¨å•è‡ªåŠ¨åŒ–**:

```python
class FormHandler:
    """æ™ºèƒ½è¡¨å•å¤„ç†å™¨"""

    async def fill_and_submit_form(self, url: str, form_data: Dict):
        """è‡ªåŠ¨è¯†åˆ«è¡¨å•å…ƒç´ ç±»å‹å¹¶å¡«å†™"""
        # æ”¯æŒ input/select/textarea/checkbox/radio ç­‰æ‰€æœ‰å…ƒç´ 
        # æ™ºèƒ½ç­‰å¾…å’Œæäº¤ç­–ç•¥
```

#### 3. ä¼ä¸šçº§å·¥å…·é›† (`extractor/utils.py`)

**æ ¸å¿ƒå·¥å…·ç±»**:

- **RateLimiter**: è¯·æ±‚é¢‘ç‡æ§åˆ¶ï¼Œé˜²æ­¢æœåŠ¡å™¨è¿‡è½½
- **RetryManager**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œæ™ºèƒ½é”™è¯¯æ¢å¤
- **CacheManager**: å†…å­˜ç¼“å­˜ç³»ç»Ÿï¼Œæå‡é‡å¤è¯·æ±‚æ€§èƒ½
- **MetricsCollector**: æ€§èƒ½æŒ‡æ ‡æ”¶é›†ï¼Œæ”¯æŒå®æ—¶ç›‘æ§
- **ErrorHandler**: é”™è¯¯åˆ†ç±»å¤„ç†ï¼ŒåŒºåˆ†ç½‘ç»œ/è¶…æ—¶/åçˆ¬ç­‰å¼‚å¸¸

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from extractor.utils import rate_limiter, retry_manager, cache_manager

# é™æµæ§åˆ¶
await rate_limiter.wait()

# æ™ºèƒ½é‡è¯•
result = await retry_manager.retry_async(scrape_function)

# ç¼“å­˜ç®¡ç†
cache_manager.set(url, result, ttl=3600)
```

#### 4. é…ç½®ç®¡ç†ç³»ç»Ÿ (`extractor/config.py`)

**DataExtractorSettings é…ç½®ç±»**:

```python
class DataExtractorSettings(BaseSettings):
    """Pydantic é…ç½®ç®¡ç†"""

    # æœåŠ¡å™¨é…ç½®
    server_name: str = "Data Extractor MCP Server"
    concurrent_requests: int = 16

    # æµè§ˆå™¨é…ç½®
    enable_javascript: bool = False
    browser_timeout: int = 30

    # åæ£€æµ‹é…ç½®
    use_random_user_agent: bool = False

    model_config = SettingsConfigDict(
        env_prefix="DATA_EXTRACTOR_",  # ç¯å¢ƒå˜é‡å‰ç¼€
        env_file=".env"
    )
```

### Data Extractor MCP å·¥å…·é›†

MCP (Model Context Protocol) å·¥å…·é›†åŸºäº FastMCP æ¡†æ¶ï¼Œæä¾› 10 ä¸ªä¸“ä¸šçº§ç½‘é¡µæŠ“å–å·¥å…·ï¼š

#### 1. æœåŠ¡å™¨æ¶æ„ (`extractor/server.py`)

**FastMCP æœåŠ¡å™¨è®¾è®¡**:

```python
from fastmcp import FastMCP

app = FastMCP(settings.server_name, version=settings.server_version)
web_scraper = WebScraper()
anti_detection_scraper = AntiDetectionScraper()

@app.tool()
async def scrape_webpage(url: str, method: str = "auto",
                        extract_config: Optional[Dict] = None) -> Dict:
    """MCP å·¥å…·è£…é¥°å™¨ï¼Œè‡ªåŠ¨å¤„ç†è¾“å…¥éªŒè¯å’Œé”™è¯¯å¤„ç†"""
```

#### 2. æ ¸å¿ƒå·¥å…·è¯¦ç»†å®ç°

**scrape_webpage - åŸºç¡€æŠ“å–å·¥å…·**:

```python
@app.tool()
async def scrape_webpage(url: str, method: str = "auto",
                        extract_config: Optional[Dict] = None,
                        wait_for_element: Optional[str] = None) -> Dict:
    """
    æ”¯æŒçš„æ•°æ®æå–é…ç½®:
    {
        "title": "h1",                          # ç®€å•é€‰æ‹©å™¨
        "products": {                           # é«˜çº§é…ç½®
            "selector": ".product",
            "multiple": true,
            "attr": "text"
        },
        "links": {
            "selector": "a",
            "multiple": true,
            "attr": "href"
        }
    }
    """
```

**scrape_with_stealth - åæ£€æµ‹å·¥å…·**:

```python
@app.tool()
async def scrape_with_stealth(url: str, method: str = "selenium",
                             extract_config: Optional[Dict] = None) -> Dict:
    """
    åæ£€æµ‹æŠ€æœ¯:
    - undetected-chromedriver: ç»•è¿‡ Selenium æ£€æµ‹
    - Playwright stealth: åŸç”Ÿåæ£€æµ‹æ”¯æŒ
    - éšæœº User-Agent: é™ä½è¯†åˆ«é£é™©
    - äººç±»è¡Œä¸ºæ¨¡æ‹Ÿ: é¼ æ ‡ç§»åŠ¨ã€é¡µé¢æ»šåŠ¨
    """
```

**fill_and_submit_form - è¡¨å•è‡ªåŠ¨åŒ–**:

```python
@app.tool()
async def fill_and_submit_form(url: str, form_data: Dict,
                              submit: bool = False) -> Dict:
    """
    æ™ºèƒ½è¡¨å•å¤„ç†:
    - è‡ªåŠ¨è¯†åˆ« input/select/textarea/checkbox å…ƒç´ 
    - æ”¯æŒå¤æ‚è¡¨å•éªŒè¯å’Œæäº¤
    - ç­‰å¾…é¡µé¢å“åº”å’Œé‡å®šå‘å¤„ç†
    """
```

## ğŸ”„ CI/CD ä¸å‘å¸ƒæµç¨‹

### è‡ªåŠ¨åŒ–å·¥ä½œæµ

é¡¹ç›®é…ç½®äº†å®Œæ•´çš„ GitHub Actions å·¥ä½œæµï¼Œæä¾›è‡ªåŠ¨åŒ–çš„æµ‹è¯•ã€æ„å»ºå’Œå‘å¸ƒåŠŸèƒ½ï¼š

#### ğŸ§ª æŒç»­é›†æˆ (CI)

- **å¤šå¹³å°æµ‹è¯•**: Ubuntu, Windows, macOS
- **å¤šç‰ˆæœ¬æ”¯æŒ**: Python 3.12, 3.13
- **ä»£ç è´¨é‡**: Ruff linting, MyPy type checking
- **å®‰å…¨æ‰«æ**: Bandit security analysis
- **è¦†ç›–ç‡æŠ¥å‘Š**: Codecov integration

#### ğŸ“¦ è‡ªåŠ¨å‘å¸ƒ

- **æ ‡ç­¾å‘å¸ƒ**: æ¨é€ `v*.*.*` æ ‡ç­¾è‡ªåŠ¨è§¦å‘å‘å¸ƒ
- **PyPI å‘å¸ƒ**: ä½¿ç”¨ OIDC trusted publishingï¼Œæ— éœ€ API å¯†é’¥
- **GitHub Releases**: è‡ªåŠ¨ç”Ÿæˆ release notes
- **æ„å»ºéªŒè¯**: å‘å¸ƒå‰å®Œæ•´æµ‹è¯•å¥—ä»¶éªŒè¯

#### ğŸ”§ ä¾èµ–ç®¡ç†

- **æ¯å‘¨æ›´æ–°**: è‡ªåŠ¨æ£€æŸ¥ä¾èµ–é¡¹æ›´æ–°
- **å®‰å…¨å®¡è®¡**: å®šæœŸå®‰å…¨æ¼æ´æ‰«æ
- **è‡ªåŠ¨ PR**: ä¾èµ–æ›´æ–°é€šè¿‡ PR æäº¤

### å‘å¸ƒæ–°ç‰ˆæœ¬

1. **æ›´æ–°ç‰ˆæœ¬å·**:

```bash
# ç¼–è¾‘ pyproject.toml
vim pyproject.toml
# æ›´æ–° version = "x.y.z"
```

2. **æ›´æ–°å˜æ›´æ—¥å¿—**:

```bash
# ç¼–è¾‘ CHANGELOG.mdï¼Œæ·»åŠ æ–°ç‰ˆæœ¬æ¡ç›®
vim CHANGELOG.md
```

3. **åˆ›å»ºå‘å¸ƒæ ‡ç­¾**:

```bash
git add pyproject.toml CHANGELOG.md
git commit -m "chore: bump version to v1.2.3"
git tag v1.2.3
git push origin main --tags
```

4. **è‡ªåŠ¨åŒ–æµç¨‹**:

- âœ… è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
- âœ… æ„å»ºåˆ†å‘åŒ…
- âœ… åˆ›å»º GitHub Release
- âœ… å‘å¸ƒåˆ° PyPI
- âœ… æ›´æ–°æ–‡æ¡£

### å¼€å‘å·¥ä½œæµ

```bash
# 1. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
git checkout -b feature/new-feature

# 2. å¼€å‘å’Œæµ‹è¯•
uv sync --extra dev
uv run pytest

# 3. ä»£ç è´¨é‡æ£€æŸ¥
uv run ruff check extractor/
uv run ruff format extractor/
uv run mypy extractor/

# 4. æäº¤PR
git push origin feature/new-feature
# åˆ›å»ºPRï¼ŒCIè‡ªåŠ¨è¿è¡Œæµ‹è¯•
```

### ç›‘æ§å’Œç»´æŠ¤

- **CI çŠ¶æ€**: [![CI](https://github.com/ThreeFish-AI/data-extractor/actions/workflows/ci.yml/badge.svg)](https://github.com/ThreeFish-AI/data-extractor/actions/workflows/ci.yml)
- **å‘å¸ƒçŠ¶æ€**: [![Release](https://github.com/ThreeFish-AI/data-extractor/actions/workflows/release.yml/badge.svg)](https://github.com/ThreeFish-AI/data-extractor/actions/workflows/release.yml)
- **ä¾èµ–æ›´æ–°**: [![Dependencies](https://github.com/ThreeFish-AI/data-extractor/actions/workflows/dependencies.yml/badge.svg)](https://github.com/ThreeFish-AI/data-extractor/actions/workflows/dependencies.yml)

## ğŸš€ å®ç°ä¸ä½¿ç”¨æŒ‡å—

### Data Extractor æ ¸å¿ƒå¼•æ“ä½¿ç”¨æ–¹å¼

#### 1. ç›´æ¥ä½¿ç”¨æ ¸å¿ƒå¼•æ“

```python
from extractor.scraper import WebScraper
from extractor.advanced_features import AntiDetectionScraper, FormHandler

# åŸºç¡€æŠ“å–
scraper = WebScraper()
result = await scraper.scrape_url("https://example.com", method="simple")

# åæ£€æµ‹æŠ“å–
stealth_scraper = AntiDetectionScraper()
result = await stealth_scraper.scrape_with_stealth("https://protected-site.com")

# è¡¨å•è‡ªåŠ¨åŒ–
form_handler = FormHandler()
result = await form_handler.fill_and_submit_form(
    "https://example.com/contact",
    {"input[name='email']": "test@example.com"}
)
```

#### 2. é…ç½®åŒ–æ•°æ®æå–

```python
# ç®€å•é…ç½®
extract_config = {
    "title": "h1",
    "content": ".article-content"
}

# é«˜çº§é…ç½®
extract_config = {
    "products": {
        "selector": ".product-item",
        "multiple": True,
        "attr": "text"
    },
    "prices": {
        "selector": ".price",
        "multiple": True,
        "attr": "data-price"
    },
    "images": {
        "selector": "img.product-image",
        "multiple": True,
        "attr": "src"
    }
}

result = await scraper.scrape_url(url, extract_config=extract_config)
```

#### 3. ä¼ä¸šçº§åŠŸèƒ½é›†æˆ

```python
from extractor.utils import (
    rate_limiter, retry_manager, cache_manager,
    metrics_collector, error_handler
)

# é›†æˆå®Œæ•´åŠŸèƒ½çš„æŠ“å–æµç¨‹
async def enterprise_scrape(url: str):
    # æ£€æŸ¥ç¼“å­˜
    cached_result = cache_manager.get(url)
    if cached_result:
        return cached_result

    # é€Ÿç‡é™åˆ¶
    await rate_limiter.wait()

    # é‡è¯•æœºåˆ¶
    try:
        result = await retry_manager.retry_async(
            scraper.scrape_url, url, method="auto"
        )

        # è®°å½•æŒ‡æ ‡
        metrics_collector.record_request("GET", True, 1500, "scraper")

        # ç¼“å­˜ç»“æœ
        cache_manager.set(url, result, ttl=3600)

        return result

    except Exception as e:
        error_handler.handle_error(e, "enterprise_scrape")
        raise
```

### Data Extractor MCP å·¥å…·é›†ä½¿ç”¨æ–¹å¼

#### 1. MCP Client é›†æˆ

**é€šè¿‡ Claude Desktop ä½¿ç”¨**:

1. å¯åŠ¨ Data Extractor MCP æœåŠ¡å™¨
2. åœ¨ Claude Desktop ä¸­é…ç½®æœåŠ¡å™¨è¿æ¥
3. ç›´æ¥è°ƒç”¨ MCP å·¥å…·è¿›è¡Œç½‘é¡µæŠ“å–

**ç¤ºä¾‹å¯¹è¯**:

```
ç”¨æˆ·: å¸®æˆ‘æŠ“å– https://news.ycombinator.com çš„æ ‡é¢˜å’Œé“¾æ¥

Claude: æˆ‘æ¥ä½¿ç”¨ scrape_webpage å·¥å…·ä¸ºæ‚¨æŠ“å– Hacker News çš„å†…å®¹

å·¥å…·è°ƒç”¨: scrape_webpage
å‚æ•°: {
  "url": "https://news.ycombinator.com",
  "extract_config": {
    "titles": {
      "selector": ".titleline > a",
      "multiple": true,
      "attr": "text"
    },
    "links": {
      "selector": ".titleline > a",
      "multiple": true,
      "attr": "href"
    }
  }
}
```

#### 2. ç¼–ç¨‹æ–¹å¼è°ƒç”¨ MCP å·¥å…·

```python
# é€šè¿‡ MCP åè®®è°ƒç”¨å·¥å…·
import asyncio
from extractor.server import (
    scrape_webpage, scrape_multiple_webpages,
    scrape_with_stealth, fill_and_submit_form
)

# åŸºç¡€é¡µé¢æŠ“å–
async def basic_scraping_example():
    result = await scrape_webpage(
        url="https://example.com",
        method="auto",
        extract_config={
            "title": "h1",
            "content": ".main-content"
        }
    )
    print(f"é¡µé¢æ ‡é¢˜: {result['data']['extracted_data']['title']}")

# æ‰¹é‡æŠ“å–
async def batch_scraping_example():
    urls = [
        "https://site1.com",
        "https://site2.com",
        "https://site3.com"
    ]

    results = await scrape_multiple_webpages(
        urls=urls,
        method="simple",
        extract_config={"title": "h1"}
    )

    for result in results['data']:
        print(f"URL: {result['url']}, æ ‡é¢˜: {result.get('title', 'N/A')}")

# åæ£€æµ‹æŠ“å–
async def stealth_scraping_example():
    result = await scrape_with_stealth(
        url="https://protected-website.com",
        method="playwright",
        extract_config={
            "content": ".protected-content",
            "data": "[data-value]"
        }
    )
    return result

# è¡¨å•è‡ªåŠ¨åŒ–
async def form_automation_example():
    result = await fill_and_submit_form(
        url="https://example.com/contact",
        form_data={
            "input[name='name']": "John Doe",
            "input[name='email']": "john@example.com",
            "textarea[name='message']": "Hello from Data Extractor!"
        },
        submit=True,
        submit_button_selector="button[type='submit']"
    )
    return result
```

#### 3. é«˜çº§ä½¿ç”¨åœºæ™¯

**ç”µå•†æ•°æ®æŠ“å–**:

```python
async def ecommerce_scraping():
    # æŠ“å–äº§å“åˆ—è¡¨
    products_result = await scrape_webpage(
        url="https://shop.example.com/products",
        extract_config={
            "products": {
                "selector": ".product-card",
                "multiple": True,
                "attr": "text"
            },
            "prices": {
                "selector": ".price",
                "multiple": True,
                "attr": "text"
            },
            "product_links": {
                "selector": ".product-card a",
                "multiple": True,
                "attr": "href"
            }
        }
    )

    # æ‰¹é‡æŠ“å–äº§å“è¯¦æƒ…
    product_urls = products_result['data']['extracted_data']['product_links']
    details = await scrape_multiple_webpages(
        urls=product_urls[:10],  # é™åˆ¶å‰10ä¸ªäº§å“
        extract_config={
            "description": ".product-description",
            "specifications": ".specs-table",
            "images": {
                "selector": ".product-images img",
                "multiple": True,
                "attr": "src"
            }
        }
    )

    return {
        "products_overview": products_result,
        "product_details": details
    }
```

**æ–°é—»ç›‘æ§ç³»ç»Ÿ**:

```python
async def news_monitoring_system():
    news_sites = [
        "https://news.ycombinator.com",
        "https://techcrunch.com",
        "https://arstechnica.com"
    ]

    # æ‰¹é‡æŠ“å–æ–°é—»æ ‡é¢˜
    news_results = await scrape_multiple_webpages(
        urls=news_sites,
        extract_config={
            "headlines": {
                "selector": "h1, h2, .headline",
                "multiple": True,
                "attr": "text"
            },
            "timestamps": {
                "selector": ".timestamp, time",
                "multiple": True,
                "attr": "text"
            }
        }
    )

    # æå–æ‰€æœ‰é“¾æ¥ç”¨äºæ·±åº¦åˆ†æ
    all_links = []
    for site in news_sites:
        links_result = await extract_links(
            url=site,
            internal_only=True
        )
        all_links.extend(links_result['data']['links'])

    return {
        "news_headlines": news_results,
        "discovered_links": all_links
    }
```

**åˆè§„æ€§æ£€æŸ¥æµç¨‹**:

```python
async def compliance_check_workflow(target_url: str):
    # 1. æ£€æŸ¥ robots.txt
    robots_result = await check_robots_txt(target_url)

    if not robots_result['data']['can_crawl']:
        return {"error": "ç½‘ç«™ç¦æ­¢çˆ¬å–", "robots_txt": robots_result}

    # 2. è·å–é¡µé¢åŸºç¡€ä¿¡æ¯
    page_info = await get_page_info(target_url)

    # 3. æ‰§è¡Œåˆè§„çš„æ•°æ®æŠ“å–
    scrape_result = await scrape_webpage(
        url=target_url,
        method="simple",  # ä½¿ç”¨æœ€è½»é‡çš„æ–¹æ³•
        extract_config={
            "public_content": ".main-content, .article",
            "meta_info": "meta[name='description']"
        }
    )

    # 4. æ£€æŸ¥æœåŠ¡å™¨æ€§èƒ½å½±å“
    metrics = await get_server_metrics()

    return {
        "compliance_check": robots_result,
        "page_info": page_info,
        "extracted_data": scrape_result,
        "performance_metrics": metrics
    }
```

## ğŸ“‹ ç‰ˆæœ¬ç®¡ç†

### é¡¹ç›®ç‰ˆæœ¬ç»´æŠ¤

é¡¹ç›®ä½¿ç”¨è¯­ä¹‰åŒ–ç‰ˆæœ¬æ§åˆ¶ï¼ˆSemantic Versioningï¼‰ï¼Œç‰ˆæœ¬å·æ ¼å¼ä¸º `MAJOR.MINOR.PATCH`ï¼š

- **MAJOR**: é‡å¤§ä¸å…¼å®¹å˜æ›´
- **MINOR**: æ–°åŠŸèƒ½å¢åŠ ï¼Œå‘åå…¼å®¹
- **PATCH**: é”™è¯¯ä¿®å¤ï¼Œå‘åå…¼å®¹

### ç‰ˆæœ¬å‡çº§æ­¥éª¤

1. **æ›´æ–°ç‰ˆæœ¬å·**

```bash
# ç¼–è¾‘ pyproject.toml ä¸­çš„ version å­—æ®µ
vim pyproject.toml
```

2. **æ›´æ–°å˜æ›´æ—¥å¿—**

```bash
# åœ¨ CHANGELOG.md ä¸­è®°å½•å˜æ›´å†…å®¹
vim CHANGELOG.md
```

3. **æ›´æ–° README ç‰ˆæœ¬ä¿¡æ¯**

```bash
# æ›´æ–° README.md ä¸­çš„"å½“å‰æœ€æ–°ç¨³å®šç‰ˆæœ¬"
vim README.md
```

4. **æäº¤ç‰ˆæœ¬å˜æ›´**

```bash
git add pyproject.toml CHANGELOG.md README.md
git commit -m "chore(release): bump version to vX.Y.Z"
git tag -a vX.Y.Z -m "Release version X.Y.Z"
git push && git push --tags
```

5. **æ„å»ºå’Œå‘å¸ƒ**

```bash
# ä½¿ç”¨ uv æ„å»ºåŒ…
uv build

# å‘å¸ƒåˆ° PyPIï¼ˆå¦‚éœ€è¦ï¼‰
uv publish
```

### ç‰ˆæœ¬æ£€æŸ¥

```bash
# æ£€æŸ¥å½“å‰ç‰ˆæœ¬
python -c "import extractor; print(extractor.__version__)"

# æˆ–ä½¿ç”¨ uv
uv run python -c "from extractor import __version__; print(__version__)"
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„æ–¹æ³•

- **simple**: é™æ€å†…å®¹ï¼Œå¿«é€Ÿçˆ¬å–
- **scrapy**: å¤§è§„æ¨¡çˆ¬å–ï¼Œéœ€è¦é«˜çº§ç‰¹æ€§
- **selenium**: JavaScript é‡åº¦ç½‘ç«™
- **stealth**: æœ‰åçˆ¬ä¿æŠ¤çš„ç½‘ç«™

### 2. éµå®ˆç½‘ç«™è§„åˆ™

- ä½¿ç”¨ `check_robots_txt` å·¥å…·æ£€æŸ¥çˆ¬å–è§„åˆ™
- è®¾ç½®åˆé€‚çš„å»¶è¿Ÿå’Œå¹¶å‘é™åˆ¶
- å°Šé‡ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾

### 3. æ€§èƒ½ä¼˜åŒ–

- ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è¯·æ±‚
- åˆç†è®¾ç½®è¶…æ—¶æ—¶é—´
- ç›‘æ§ `get_server_metrics` è°ƒæ•´é…ç½®

### 4. é”™è¯¯å¤„ç†

- å®æ–½é‡è¯•é€»è¾‘
- ç›‘æ§é”™è¯¯ç±»åˆ«
- æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­–ç•¥

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. Selenium/Playwright å¯åŠ¨å¤±è´¥**

- ç¡®ä¿å®‰è£…äº† Chrome æµè§ˆå™¨
- æ£€æŸ¥ç³»ç»Ÿæƒé™å’Œé˜²ç«å¢™è®¾ç½®

**2. åçˆ¬è™«æ£€æµ‹**

- ä½¿ç”¨ `scrape_with_stealth` å·¥å…·
- å¯ç”¨éšæœº User-Agent
- é…ç½®ä»£ç†æœåŠ¡å™¨

**3. è¶…æ—¶é”™è¯¯**

- å¢åŠ  `browser_timeout` è®¾ç½®
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ä½¿ç”¨æ›´ç¨³å®šçš„çˆ¬å–æ–¹æ³•

**4. å†…å­˜å ç”¨è¿‡é«˜**

- å‡å°‘å¹¶å‘è¯·æ±‚æ•°
- æ¸…ç†ç¼“å­˜
- æ£€æŸ¥æ˜¯å¦æœ‰èµ„æºæ³„éœ²

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

ä½¿ç”¨ `get_server_metrics` å·¥å…·ç›‘æ§ï¼š

- è¯·æ±‚æ€»æ•°å’ŒæˆåŠŸç‡
- å¹³å‡å“åº”æ—¶é—´
- é”™è¯¯åˆ†ç±»ç»Ÿè®¡
- æ–¹æ³•ä½¿ç”¨åˆ†å¸ƒ
- ç¼“å­˜å‘½ä¸­ç‡

## ğŸ”’ å®‰å…¨æ³¨æ„äº‹é¡¹

- ä¸è¦åœ¨æ—¥å¿—ä¸­è®°å½•æ•æ„Ÿä¿¡æ¯
- ä½¿ç”¨ HTTPS ä»£ç†æœåŠ¡å™¨
- å®šæœŸæ›´æ–°ä¾èµ–åŒ…
- éµå®ˆæ•°æ®ä¿æŠ¤æ³•è§„

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ã€‚

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ LICENSE æ–‡ä»¶

## ğŸ“ æ”¯æŒ

å¦‚é‡é—®é¢˜è¯·æäº¤ GitHub Issue æˆ–è”ç³» [@ThreeFish-AI](aureliusshu@gmail.com)ã€‚

---

**æ³¨æ„**: è¯·è´Ÿè´£ä»»åœ°ä½¿ç”¨æ­¤å·¥å…·ï¼Œéµå®ˆç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾å’Œ robots.txt è§„åˆ™ï¼Œå°Šé‡ç½‘ç«™çš„çŸ¥è¯†äº§æƒã€‚
