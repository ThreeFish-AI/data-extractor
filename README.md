# Data Extractor

ä¸€ä¸ªåŸºäº Scrapy å’Œ FastMCP æ„å»ºçš„å¼ºå¤§ã€ç¨³å®šçš„ç½‘é¡µçˆ¬å–ä¸æ•°æ®æå– MCP Serverï¼Œä¸“ä¸ºå•†ä¸šç¯å¢ƒä¸­çš„é•¿æœŸä½¿ç”¨è€Œè®¾è®¡ã€‚

## ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„

```
data-extractor/
â”œâ”€â”€ extractor/                          # æ ¸å¿ƒå¼•æ“æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py                     # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ server.py                       # FastMCP æœåŠ¡å™¨ä¸ 10 ä¸ª MCP å·¥å…·
â”‚   â”œâ”€â”€ scraper.py                      # WebScraper æ ¸å¿ƒæŠ“å–å¼•æ“
â”‚   â”œâ”€â”€ advanced_features.py            # åæ£€æµ‹ä¸è¡¨å•è‡ªåŠ¨åŒ–
â”‚   â”œâ”€â”€ config.py                       # é…ç½®ç®¡ç† (DataExtractorSettings)
â”‚   â””â”€â”€ utils.py                        # ä¼ä¸šçº§å·¥å…·é›† (é™æµã€é‡è¯•ã€ç¼“å­˜ç­‰)
â”œâ”€â”€ examples/                           # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ basic_usage.py                  # åŸºç¡€ç”¨æ³•ç¤ºä¾‹
â”‚   â””â”€â”€ extraction_configs.py           # æ•°æ®æå–é…ç½®ç¤ºä¾‹
â”œâ”€â”€ tests/                              # å®Œæ•´æµ‹è¯•ä½“ç³»
â”‚   â”œâ”€â”€ unit/                           # å•å…ƒæµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_scraper_simple.py      # WebScraper åŸºç¡€æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_utils_basic.py         # å·¥å…·ç±»æµ‹è¯•
â”‚   â”‚   â””â”€â”€ test_advanced_features.py   # é«˜çº§åŠŸèƒ½æµ‹è¯•
â”‚   â”œâ”€â”€ integration/                    # é›†æˆæµ‹è¯•
â”‚   â”‚   â””â”€â”€ test_mcp_tools.py           # 10 ä¸ª MCP å·¥å…·æµ‹è¯•
â”‚   â””â”€â”€ conftest.py                     # pytest é…ç½®å’Œ fixtures
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup.sh                        # å¿«é€Ÿå®‰è£…è„šæœ¬
â”œâ”€â”€ TESTING.md                          # æµ‹è¯•æ–‡æ¡£ (67KB)
â”œâ”€â”€ TEST_RESULTS.md                     # æµ‹è¯•æ‰§è¡ŒæŠ¥å‘Š
â”œâ”€â”€ CHANGELOG.md                        # ç‰ˆæœ¬å˜æ›´æ—¥å¿—
â”œâ”€â”€ CLAUDE.md                           # Claude Code é¡¹ç›®æŒ‡å¯¼
â”œâ”€â”€ .env.example                        # ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹
â”œâ”€â”€ pyproject.toml                      # é¡¹ç›®é…ç½®å’Œä¾èµ–ç®¡ç†
â””â”€â”€ uv.lock                             # ä¾èµ–é”å®šæ–‡ä»¶
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### DataExtractor æ ¸å¿ƒå¼•æ“å±‚

DataExtractor æ ¸å¿ƒå¼•æ“é‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œæä¾›ç¨³å®šå¯é çš„ç½‘é¡µæŠ“å–èƒ½åŠ›ï¼š

#### 1. WebScraper ä¸»æ§åˆ¶å™¨ (`extractor/scraper.py`)

**è®¾è®¡ç†å¿µ**: ç»Ÿä¸€æ¥å£ï¼Œæ™ºèƒ½æ–¹æ³•é€‰æ‹©

```python
class WebScraper:
    """ä¸»æ§åˆ¶å™¨ï¼Œåè°ƒå„ç§æŠ“å–æ–¹æ³•"""

    def __init__(self):
        self.scrapy_wrapper = ScrapyWrapper()      # Scrapy æ¡†æ¶å°è£…
        self.selenium_scraper = SeleniumScraper()  # æµè§ˆå™¨è‡ªåŠ¨åŒ–
        self.simple_scraper = SimpleScraper()      # HTTP è¯·æ±‚

    async def scrape_url(self, url: str, method: str = "auto",
                         extract_config: Optional[Dict] = None) -> Dict:
        """æ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„æŠ“å–æ–¹æ³•"""
```

**æ ¸å¿ƒç‰¹æ€§**:

- **æ–¹æ³•è‡ªé€‰**: æ ¹æ® JavaScript éœ€æ±‚å’Œåæ£€æµ‹è¦æ±‚è‡ªåŠ¨é€‰æ‹© simple/scrapy/selenium
- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æŠ“å–æ–¹æ³•é€šè¿‡ç»Ÿä¸€çš„ `scrape_url()` æ¥å£è°ƒç”¨
- **å¹¶å‘æ”¯æŒ**: `scrape_multiple_urls()` å®ç°é«˜æ•ˆæ‰¹é‡å¤„ç†
- **é…ç½®åŒ–æå–**: æ”¯æŒ CSS é€‰æ‹©å™¨ã€å±æ€§æå–ã€å¤šå…ƒç´ æ‰¹é‡è·å–

#### 2. é«˜çº§åŠŸèƒ½æ¨¡å— (`extractor/advanced_features.py`)

**AntiDetectionScraper åæ£€æµ‹å¼•æ“**:

```python
class AntiDetectionScraper:
    """åæ£€æµ‹ä¸“ç”¨æŠ“å–å™¨"""

    async def scrape_with_stealth(self, url: str, method: str = "selenium"):
        """ä½¿ç”¨åæ£€æµ‹æŠ€æœ¯æŠ“å–"""
        # æ”¯æŒ undetected-chromedriver å’Œ Playwright åŒå¼•æ“
        # è‡ªåŠ¨æ³¨å…¥éšèº«è„šæœ¬ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
```

**FormHandler è¡¨å•è‡ªåŠ¨åŒ–**:

```python
class FormHandler:
    """æ™ºèƒ½è¡¨å•å¤„ç†å™¨"""

    async def fill_and_submit_form(self, url: str, form_data: Dict):
        """è‡ªåŠ¨è¯†åˆ«è¡¨å•å…ƒç´ ç±»å‹å¹¶å¡«å†™"""
        # æ”¯æŒ input/select/textarea/checkbox/radio ç­‰æ‰€æœ‰å…ƒç´ 
        # æ™ºèƒ½ç­‰å¾…å’Œæäº¤ç­–ç•¥
```

#### 3. ä¼ä¸šçº§å·¥å…·é›† (`extractor/utils.py`)

**æ ¸å¿ƒå·¥å…·ç±»**:

- **RateLimiter**: è¯·æ±‚é¢‘ç‡æ§åˆ¶ï¼Œé˜²æ­¢æœåŠ¡å™¨è¿‡è½½
- **RetryManager**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œæ™ºèƒ½é”™è¯¯æ¢å¤
- **CacheManager**: å†…å­˜ç¼“å­˜ç³»ç»Ÿï¼Œæå‡é‡å¤è¯·æ±‚æ€§èƒ½
- **MetricsCollector**: æ€§èƒ½æŒ‡æ ‡æ”¶é›†ï¼Œæ”¯æŒå®æ—¶ç›‘æ§
- **ErrorHandler**: é”™è¯¯åˆ†ç±»å¤„ç†ï¼ŒåŒºåˆ†ç½‘ç»œ/è¶…æ—¶/åçˆ¬ç­‰å¼‚å¸¸

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from extractor.utils import rate_limiter, retry_manager, cache_manager

# é™æµæ§åˆ¶
await rate_limiter.wait()

# æ™ºèƒ½é‡è¯•
result = await retry_manager.retry_async(scrape_function)

# ç¼“å­˜ç®¡ç†
cache_manager.set(url, result, ttl=3600)
```

#### 4. é…ç½®ç®¡ç†ç³»ç»Ÿ (`extractor/config.py`)

**DataExtractorSettings é…ç½®ç±»**:

```python
class DataExtractorSettings(BaseSettings):
    """Pydantic é…ç½®ç®¡ç†"""

    # æœåŠ¡å™¨é…ç½®
    server_name: str = "Data Extractor MCP Server"
    concurrent_requests: int = 16

    # æµè§ˆå™¨é…ç½®
    enable_javascript: bool = False
    browser_timeout: int = 30

    # åæ£€æµ‹é…ç½®
    use_random_user_agent: bool = False

    model_config = SettingsConfigDict(
        env_prefix="DATA_EXTRACTOR_",  # ç¯å¢ƒå˜é‡å‰ç¼€
        env_file=".env"
    )
```

### DataExtractor MCP å·¥å…·é›†

MCP (Model Context Protocol) å·¥å…·é›†åŸºäº FastMCP æ¡†æ¶ï¼Œæä¾› 10 ä¸ªä¸“ä¸šçº§ç½‘é¡µæŠ“å–å·¥å…·ï¼š

#### 1. æœåŠ¡å™¨æ¶æ„ (`extractor/server.py`)

**FastMCP æœåŠ¡å™¨è®¾è®¡**:

```python
from fastmcp import FastMCP

app = FastMCP(settings.server_name, version=settings.server_version)
web_scraper = WebScraper()
anti_detection_scraper = AntiDetectionScraper()

@app.tool()
async def scrape_webpage(url: str, method: str = "auto",
                        extract_config: Optional[Dict] = None) -> Dict:
    """MCP å·¥å…·è£…é¥°å™¨ï¼Œè‡ªåŠ¨å¤„ç†è¾“å…¥éªŒè¯å’Œé”™è¯¯å¤„ç†"""
```

#### 2. 10 ä¸ªæ ¸å¿ƒ MCP å·¥å…·

| å·¥å…·åç§°                     | åŠŸèƒ½æè¿°       | ä½¿ç”¨åœºæ™¯                         |
| ---------------------------- | -------------- | -------------------------------- |
| **scrape_webpage**           | å•é¡µé¢æŠ“å–     | åŸºç¡€æ•°æ®æå–ï¼Œæ”¯æŒé…ç½®åŒ–é€‰æ‹©å™¨   |
| **scrape_multiple_webpages** | æ‰¹é‡é¡µé¢æŠ“å–   | å¹¶å‘å¤„ç†å¤šä¸ª URLï¼Œæå‡æ•ˆç‡       |
| **scrape_with_stealth**      | åæ£€æµ‹æŠ“å–     | åº”å¯¹åçˆ¬è™«ä¿æŠ¤çš„é«˜éš¾åº¦ç½‘ç«™       |
| **fill_and_submit_form**     | è¡¨å•è‡ªåŠ¨åŒ–     | ç™»å½•è¡¨å•ã€è”ç³»è¡¨å•ç­‰äº¤äº’æ“ä½œ     |
| **extract_links**            | ä¸“ä¸šé“¾æ¥æå–   | ç½‘ç«™åœ°å›¾ç”Ÿæˆï¼Œé“¾æ¥åˆ†æ           |
| **extract_structured_data**  | ç»“æ„åŒ–æ•°æ®æå– | JSON-LDã€å¾®æ•°æ®ã€Open Graph è§£æ |
| **get_page_info**            | é¡µé¢ä¿¡æ¯è·å–   | å¿«é€Ÿè·å–æ ‡é¢˜ã€çŠ¶æ€ç ã€å…ƒæ•°æ®     |
| **check_robots_txt**         | çˆ¬è™«è§„åˆ™æ£€æŸ¥   | éµå®ˆç½‘ç«™çˆ¬å–è§„èŒƒï¼Œåˆè§„æ€§æ£€æŸ¥     |
| **get_server_metrics**       | æ€§èƒ½æŒ‡æ ‡ç›‘æ§   | æœåŠ¡å™¨çŠ¶æ€ç›‘æ§ï¼Œæ€§èƒ½è°ƒä¼˜         |
| **clear_cache**              | ç¼“å­˜ç®¡ç†       | é‡Šæ”¾å†…å­˜ï¼Œæ¸…ç†è¿‡æœŸæ•°æ®           |

#### 3. æ ¸å¿ƒå·¥å…·è¯¦ç»†å®ç°

**scrape_webpage - åŸºç¡€æŠ“å–å·¥å…·**:

```python
@app.tool()
async def scrape_webpage(url: str, method: str = "auto",
                        extract_config: Optional[Dict] = None,
                        wait_for_element: Optional[str] = None) -> Dict:
    """
    æ”¯æŒçš„æ•°æ®æå–é…ç½®:
    {
        "title": "h1",                          # ç®€å•é€‰æ‹©å™¨
        "products": {                           # é«˜çº§é…ç½®
            "selector": ".product",
            "multiple": true,
            "attr": "text"
        },
        "links": {
            "selector": "a",
            "multiple": true,
            "attr": "href"
        }
    }
    """
```

**scrape_with_stealth - åæ£€æµ‹å·¥å…·**:

```python
@app.tool()
async def scrape_with_stealth(url: str, method: str = "selenium",
                             extract_config: Optional[Dict] = None) -> Dict:
    """
    åæ£€æµ‹æŠ€æœ¯:
    - undetected-chromedriver: ç»•è¿‡ Selenium æ£€æµ‹
    - Playwright stealth: åŸç”Ÿåæ£€æµ‹æ”¯æŒ
    - éšæœº User-Agent: é™ä½è¯†åˆ«é£é™©
    - äººç±»è¡Œä¸ºæ¨¡æ‹Ÿ: é¼ æ ‡ç§»åŠ¨ã€é¡µé¢æ»šåŠ¨
    """
```

**fill_and_submit_form - è¡¨å•è‡ªåŠ¨åŒ–**:

```python
@app.tool()
async def fill_and_submit_form(url: str, form_data: Dict,
                              submit: bool = False) -> Dict:
    """
    æ™ºèƒ½è¡¨å•å¤„ç†:
    - è‡ªåŠ¨è¯†åˆ« input/select/textarea/checkbox å…ƒç´ 
    - æ”¯æŒå¤æ‚è¡¨å•éªŒè¯å’Œæäº¤
    - ç­‰å¾…é¡µé¢å“åº”å’Œé‡å®šå‘å¤„ç†
    """
```

## ğŸš€ å®ç°æ–¹å¼ä¸ä½¿ç”¨æŒ‡å—

### DataExtractor æ ¸å¿ƒå¼•æ“ä½¿ç”¨æ–¹å¼

#### 1. ç›´æ¥ä½¿ç”¨æ ¸å¿ƒå¼•æ“

```python
from extractor.scraper import WebScraper
from extractor.advanced_features import AntiDetectionScraper, FormHandler

# åŸºç¡€æŠ“å–
scraper = WebScraper()
result = await scraper.scrape_url("https://example.com", method="simple")

# åæ£€æµ‹æŠ“å–
stealth_scraper = AntiDetectionScraper()
result = await stealth_scraper.scrape_with_stealth("https://protected-site.com")

# è¡¨å•è‡ªåŠ¨åŒ–
form_handler = FormHandler()
result = await form_handler.fill_and_submit_form(
    "https://example.com/contact",
    {"input[name='email']": "test@example.com"}
)
```

#### 2. é…ç½®åŒ–æ•°æ®æå–

```python
# ç®€å•é…ç½®
extract_config = {
    "title": "h1",
    "content": ".article-content"
}

# é«˜çº§é…ç½®
extract_config = {
    "products": {
        "selector": ".product-item",
        "multiple": True,
        "attr": "text"
    },
    "prices": {
        "selector": ".price",
        "multiple": True,
        "attr": "data-price"
    },
    "images": {
        "selector": "img.product-image",
        "multiple": True,
        "attr": "src"
    }
}

result = await scraper.scrape_url(url, extract_config=extract_config)
```

#### 3. ä¼ä¸šçº§åŠŸèƒ½é›†æˆ

```python
from extractor.utils import (
    rate_limiter, retry_manager, cache_manager,
    metrics_collector, error_handler
)

# é›†æˆå®Œæ•´åŠŸèƒ½çš„æŠ“å–æµç¨‹
async def enterprise_scrape(url: str):
    # æ£€æŸ¥ç¼“å­˜
    cached_result = cache_manager.get(url)
    if cached_result:
        return cached_result

    # é€Ÿç‡é™åˆ¶
    await rate_limiter.wait()

    # é‡è¯•æœºåˆ¶
    try:
        result = await retry_manager.retry_async(
            scraper.scrape_url, url, method="auto"
        )

        # è®°å½•æŒ‡æ ‡
        metrics_collector.record_request("GET", True, 1500, "scraper")

        # ç¼“å­˜ç»“æœ
        cache_manager.set(url, result, ttl=3600)

        return result

    except Exception as e:
        error_handler.handle_error(e, "enterprise_scrape")
        raise
```

### DataExtractor MCP å·¥å…·é›†ä½¿ç”¨æ–¹å¼

#### 1. MCP Client é›†æˆ

**é€šè¿‡ Claude Desktop ä½¿ç”¨**:

1. å¯åŠ¨ Data Extractor MCP æœåŠ¡å™¨
2. åœ¨ Claude Desktop ä¸­é…ç½®æœåŠ¡å™¨è¿æ¥
3. ç›´æ¥è°ƒç”¨ MCP å·¥å…·è¿›è¡Œç½‘é¡µæŠ“å–

**ç¤ºä¾‹å¯¹è¯**:

```
ç”¨æˆ·: å¸®æˆ‘æŠ“å– https://news.ycombinator.com çš„æ ‡é¢˜å’Œé“¾æ¥

Claude: æˆ‘æ¥ä½¿ç”¨ scrape_webpage å·¥å…·ä¸ºæ‚¨æŠ“å– Hacker News çš„å†…å®¹

å·¥å…·è°ƒç”¨: scrape_webpage
å‚æ•°: {
  "url": "https://news.ycombinator.com",
  "extract_config": {
    "titles": {
      "selector": ".titleline > a",
      "multiple": true,
      "attr": "text"
    },
    "links": {
      "selector": ".titleline > a",
      "multiple": true,
      "attr": "href"
    }
  }
}
```

#### 2. ç¼–ç¨‹æ–¹å¼è°ƒç”¨ MCP å·¥å…·

```python
# é€šè¿‡ MCP åè®®è°ƒç”¨å·¥å…·
import asyncio
from extractor.server import (
    scrape_webpage, scrape_multiple_webpages,
    scrape_with_stealth, fill_and_submit_form
)

# åŸºç¡€é¡µé¢æŠ“å–
async def basic_scraping_example():
    result = await scrape_webpage(
        url="https://example.com",
        method="auto",
        extract_config={
            "title": "h1",
            "content": ".main-content"
        }
    )
    print(f"é¡µé¢æ ‡é¢˜: {result['data']['extracted_data']['title']}")

# æ‰¹é‡æŠ“å–
async def batch_scraping_example():
    urls = [
        "https://site1.com",
        "https://site2.com",
        "https://site3.com"
    ]

    results = await scrape_multiple_webpages(
        urls=urls,
        method="simple",
        extract_config={"title": "h1"}
    )

    for result in results['data']:
        print(f"URL: {result['url']}, æ ‡é¢˜: {result.get('title', 'N/A')}")

# åæ£€æµ‹æŠ“å–
async def stealth_scraping_example():
    result = await scrape_with_stealth(
        url="https://protected-website.com",
        method="playwright",
        extract_config={
            "content": ".protected-content",
            "data": "[data-value]"
        }
    )
    return result

# è¡¨å•è‡ªåŠ¨åŒ–
async def form_automation_example():
    result = await fill_and_submit_form(
        url="https://example.com/contact",
        form_data={
            "input[name='name']": "John Doe",
            "input[name='email']": "john@example.com",
            "textarea[name='message']": "Hello from Data Extractor!"
        },
        submit=True,
        submit_button_selector="button[type='submit']"
    )
    return result
```

#### 3. é«˜çº§ä½¿ç”¨åœºæ™¯

**ç”µå•†æ•°æ®æŠ“å–**:

```python
async def ecommerce_scraping():
    # æŠ“å–äº§å“åˆ—è¡¨
    products_result = await scrape_webpage(
        url="https://shop.example.com/products",
        extract_config={
            "products": {
                "selector": ".product-card",
                "multiple": True,
                "attr": "text"
            },
            "prices": {
                "selector": ".price",
                "multiple": True,
                "attr": "text"
            },
            "product_links": {
                "selector": ".product-card a",
                "multiple": True,
                "attr": "href"
            }
        }
    )

    # æ‰¹é‡æŠ“å–äº§å“è¯¦æƒ…
    product_urls = products_result['data']['extracted_data']['product_links']
    details = await scrape_multiple_webpages(
        urls=product_urls[:10],  # é™åˆ¶å‰10ä¸ªäº§å“
        extract_config={
            "description": ".product-description",
            "specifications": ".specs-table",
            "images": {
                "selector": ".product-images img",
                "multiple": True,
                "attr": "src"
            }
        }
    )

    return {
        "products_overview": products_result,
        "product_details": details
    }
```

**æ–°é—»ç›‘æ§ç³»ç»Ÿ**:

```python
async def news_monitoring_system():
    news_sites = [
        "https://news.ycombinator.com",
        "https://techcrunch.com",
        "https://arstechnica.com"
    ]

    # æ‰¹é‡æŠ“å–æ–°é—»æ ‡é¢˜
    news_results = await scrape_multiple_webpages(
        urls=news_sites,
        extract_config={
            "headlines": {
                "selector": "h1, h2, .headline",
                "multiple": True,
                "attr": "text"
            },
            "timestamps": {
                "selector": ".timestamp, time",
                "multiple": True,
                "attr": "text"
            }
        }
    )

    # æå–æ‰€æœ‰é“¾æ¥ç”¨äºæ·±åº¦åˆ†æ
    all_links = []
    for site in news_sites:
        links_result = await extract_links(
            url=site,
            internal_only=True
        )
        all_links.extend(links_result['data']['links'])

    return {
        "news_headlines": news_results,
        "discovered_links": all_links
    }
```

**åˆè§„æ€§æ£€æŸ¥æµç¨‹**:

```python
async def compliance_check_workflow(target_url: str):
    # 1. æ£€æŸ¥ robots.txt
    robots_result = await check_robots_txt(target_url)

    if not robots_result['data']['can_crawl']:
        return {"error": "ç½‘ç«™ç¦æ­¢çˆ¬å–", "robots_txt": robots_result}

    # 2. è·å–é¡µé¢åŸºç¡€ä¿¡æ¯
    page_info = await get_page_info(target_url)

    # 3. æ‰§è¡Œåˆè§„çš„æ•°æ®æŠ“å–
    scrape_result = await scrape_webpage(
        url=target_url,
        method="simple",  # ä½¿ç”¨æœ€è½»é‡çš„æ–¹æ³•
        extract_config={
            "public_content": ".main-content, .article",
            "meta_info": "meta[name='description']"
        }
    )

    # 4. æ£€æŸ¥æœåŠ¡å™¨æ€§èƒ½å½±å“
    metrics = await get_server_metrics()

    return {
        "compliance_check": robots_result,
        "page_info": page_info,
        "extracted_data": scrape_result,
        "performance_metrics": metrics
    }
```

## ğŸ“‹ ç‰ˆæœ¬å†å²

### v0.1.2 (2025-09-06)

- **æµ‹è¯•æ¡†æ¶**: å»ºç«‹å®Œæ•´çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•ä½“ç³»ï¼Œ19 ä¸ªåŸºç¡€æµ‹è¯•å…¨éƒ¨é€šè¿‡
- **æµ‹è¯•æ–‡æ¡£**: æ–°å¢ 67KB è¯¦ç»†æµ‹è¯•æ–‡æ¡£å’Œæ‰§è¡ŒæŠ¥å‘Š
- **è´¨é‡ä¿éšœ**: pytest å¼‚æ­¥æµ‹è¯•æ”¯æŒï¼ŒMock ç­–ç•¥å’Œæ€§èƒ½ä¼˜åŒ–

### v0.1.1 (2025-09-05)

- **æ ¸å¿ƒé‡æ„**: åŒ…åä» `scrapy_mcp` é‡æ„ä¸º `extractor`ï¼Œæå‡é¡¹ç›®ç»“æ„æ¸…æ™°åº¦
- **å‘½ä»¤æ›´æ–°**: é¡¹ç›®å…¥å£å‘½ä»¤ç»Ÿä¸€ä¸º `data-extractor`
- **æ–‡æ¡£å®Œå–„**: æ›´æ–°æ‰€æœ‰é…ç½®ç¤ºä¾‹å’Œå®‰è£…è¯´æ˜

### v0.1.0 (2025-08-26)

- **åˆå§‹å‘å¸ƒ**: å®Œæ•´çš„ç½‘é¡µçˆ¬å– MCP Server å®ç°
- **æ ¸å¿ƒåŠŸèƒ½**: 10 ä¸ªä¸“ä¸šçˆ¬å–å·¥å…·ï¼Œæ”¯æŒå¤šç§åœºæ™¯
- **ä¼ä¸šç‰¹æ€§**: é€Ÿç‡é™åˆ¶ã€æ™ºèƒ½é‡è¯•ã€ç¼“å­˜æœºåˆ¶
- **æŠ€æœ¯æ ˆ**: è¿ç§»è‡³ uv åŒ…ç®¡ç†ï¼Œå¢å¼ºå¼€å‘ä½“éªŒ

## ğŸš€ ç‰¹æ€§

### æ ¸å¿ƒåŠŸèƒ½

- **å¤šç§çˆ¬å–æ–¹æ³•**: æ”¯æŒç®€å• HTTP è¯·æ±‚ã€Scrapy æ¡†æ¶å’Œæµè§ˆå™¨è‡ªåŠ¨åŒ–
- **æ™ºèƒ½æ–¹æ³•é€‰æ‹©**: è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„çˆ¬å–æ–¹æ³•
- **å¹¶å‘å¤„ç†**: æ”¯æŒå¤šä¸ª URL çš„å¹¶å‘çˆ¬å–
- **é…ç½®åŒ–æå–**: çµæ´»çš„æ•°æ®æå–é…ç½®ç³»ç»Ÿ

### é«˜çº§åŠŸèƒ½

- **ååçˆ¬è™«**: ä½¿ç”¨ undetected-chromedriver å’Œ Playwright çš„éšèº«æŠ€æœ¯
- **è¡¨å•å¤„ç†**: è‡ªåŠ¨å¡«å†™å’Œæäº¤å„ç§ç±»å‹çš„è¡¨å•
- **JavaScript æ”¯æŒ**: å®Œæ•´çš„æµè§ˆå™¨æ¸²æŸ“æ”¯æŒ
- **æ™ºèƒ½é‡è¯•**: æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
- **ç»“æœç¼“å­˜**: å†…å­˜ç¼“å­˜æå‡æ€§èƒ½

### ä¼ä¸šçº§ç‰¹æ€§

- **é”™è¯¯å¤„ç†**: å®Œå–„çš„é”™è¯¯åˆ†ç±»å’Œå¤„ç†
- **æ€§èƒ½ç›‘æ§**: è¯¦ç»†çš„è¯·æ±‚æŒ‡æ ‡å’Œç»Ÿè®¡
- **é€Ÿç‡é™åˆ¶**: é˜²æ­¢æœåŠ¡å™¨è¿‡è½½
- **ä»£ç†æ”¯æŒ**: æ”¯æŒ HTTP ä»£ç†é…ç½®
- **éšæœº UA**: é˜²æ£€æµ‹çš„ç”¨æˆ·ä»£ç†è½®æ¢

## ğŸ“¦ å®‰è£…

```bash
# ç¡®è®¤ Python ç‰ˆæœ¬ (éœ€è¦ 3.12+)
python --version

# å…‹éš†ä»“åº“
git clone https://github.com/ThreeFish-AI/data-extractor.git
cd data-extractor

# å¿«é€Ÿè®¾ç½®ï¼ˆæ¨èï¼‰
./scripts/setup.sh

# æˆ–æ‰‹åŠ¨å®‰è£…
# ä½¿ç”¨ uv å®‰è£…ä¾èµ–
uv sync

# å®‰è£…åŒ…æ‹¬å¼€å‘ä¾èµ–
uv sync --extra dev

# æˆ–è€…ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
pip install -e .

# æˆ–è€…ä½¿ç”¨å¼€å‘æ¨¡å¼
pip install -e ".[dev]"
```

## ğŸ“‹ ç‰ˆæœ¬ç®¡ç†

### é¡¹ç›®ç‰ˆæœ¬ç»´æŠ¤

é¡¹ç›®ä½¿ç”¨è¯­ä¹‰åŒ–ç‰ˆæœ¬æ§åˆ¶ï¼ˆSemantic Versioningï¼‰ï¼Œç‰ˆæœ¬å·æ ¼å¼ä¸º `MAJOR.MINOR.PATCH`ï¼š

- **MAJOR**: é‡å¤§ä¸å…¼å®¹å˜æ›´
- **MINOR**: æ–°åŠŸèƒ½å¢åŠ ï¼Œå‘åå…¼å®¹
- **PATCH**: é”™è¯¯ä¿®å¤ï¼Œå‘åå…¼å®¹

### ç‰ˆæœ¬å‡çº§æ­¥éª¤

1. **æ›´æ–°ç‰ˆæœ¬å·**

   ```bash
   # ç¼–è¾‘ pyproject.toml ä¸­çš„ version å­—æ®µ
   vim pyproject.toml
   ```

2. **æ›´æ–°å˜æ›´æ—¥å¿—**

   ```bash
   # åœ¨ CHANGELOG.md ä¸­è®°å½•å˜æ›´å†…å®¹
   vim CHANGELOG.md
   ```

3. **æ›´æ–° README ç‰ˆæœ¬ä¿¡æ¯**

   ```bash
   # æ›´æ–° README.md ä¸­çš„"å½“å‰æœ€æ–°ç¨³å®šç‰ˆæœ¬"
   vim README.md
   ```

4. **æäº¤ç‰ˆæœ¬å˜æ›´**

   ```bash
   git add pyproject.toml CHANGELOG.md README.md
   git commit -m "chore(release): bump version to vX.Y.Z"
   git tag -a vX.Y.Z -m "Release version X.Y.Z"
   git push && git push --tags
   ```

5. **æ„å»ºå’Œå‘å¸ƒ**

   ```bash
   # ä½¿ç”¨ uv æ„å»ºåŒ…
   uv build

   # å‘å¸ƒåˆ° PyPIï¼ˆå¦‚éœ€è¦ï¼‰
   uv publish
   ```

### ç‰ˆæœ¬æ£€æŸ¥

```bash
# æ£€æŸ¥å½“å‰ç‰ˆæœ¬
python -c "import extractor; print(extractor.__version__)"

# æˆ–ä½¿ç”¨ uv
uv run python -c "from extractor import __version__; print(__version__)"
```

## ğŸ”§ é…ç½®

åˆ›å»º `.env` æ–‡ä»¶æ¥è‡ªå®šä¹‰é…ç½®ï¼š

```bash
# æœåŠ¡å™¨è®¾ç½®
DATA_EXTRACTOR_SERVER_NAME=data-extractor
DATA_EXTRACTOR_SERVER_VERSION=0.1.2

# å¹¶å‘å’Œå»¶è¿Ÿè®¾ç½®
DATA_EXTRACTOR_CONCURRENT_REQUESTS=16
DATA_EXTRACTOR_DOWNLOAD_DELAY=1.0
DATA_EXTRACTOR_RANDOMIZE_DOWNLOAD_DELAY=true

# æµè§ˆå™¨è®¾ç½®
DATA_EXTRACTOR_ENABLE_JAVASCRIPT=false
DATA_EXTRACTOR_BROWSER_HEADLESS=true
DATA_EXTRACTOR_BROWSER_TIMEOUT=30

# åæ£€æµ‹è®¾ç½®
DATA_EXTRACTOR_USE_RANDOM_USER_AGENT=true
DATA_EXTRACTOR_USE_PROXY=false
DATA_EXTRACTOR_PROXY_URL=

# é‡è¯•è®¾ç½®
DATA_EXTRACTOR_MAX_RETRIES=3
DATA_EXTRACTOR_REQUEST_TIMEOUT=30
```

## ğŸš¦ å¿«é€Ÿå¼€å§‹

### å¯åŠ¨æœåŠ¡å™¨

```bash
# ä½¿ç”¨å‘½ä»¤è¡Œ
data-extractor

# ä½¿ç”¨ uv è¿è¡Œï¼ˆæ¨èï¼‰
uv run data-extractor

# æˆ–è€…ä½¿ç”¨Python
python -m extractor.server

# ä½¿ç”¨ uv è¿è¡Œ Python æ¨¡å—
uv run python -m extractor.server
```

### MCP Client é…ç½®

åœ¨æ‚¨çš„ MCP client (å¦‚ Claude Desktop) ä¸­æ·»åŠ æœåŠ¡å™¨é…ç½®ï¼š

#### æ–¹å¼ä¸€ï¼šç›´æ¥å‘½ä»¤æ–¹å¼

```json
{
  "mcpServers": {
    "data-extractor": {
      "command": "data-extractor",
      "args": []
    }
  }
}
```

#### æ–¹å¼äºŒï¼šé€šè¿‡ uv å¯åŠ¨ï¼ˆæ¨èï¼‰

```json
{
  "mcpServers": {
    "data-extractor": {
      "command": "uv",
      "args": ["run", "data-extractor"],
      "cwd": "/path/to/your/data-extractor"
    }
  }
}
```

#### æ–¹å¼ä¸‰ï¼šä» GitHub ä»“åº“ç›´æ¥å®‰è£…å’Œè¿è¡Œï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

```json
{
  "mcpServers": {
    "data-extractor": {
      "command": "uv",
      "args": [
        "run",
        "--with",
        "git+https://github.com/ThreeFish-AI/data-extractor.git@v0.1.1",
        "data-extractor"
      ]
    }
  }
}
```

#### æ–¹å¼å››ï¼šPython æ¨¡å—æ–¹å¼ï¼ˆæœ¬åœ°å¼€å‘ï¼‰

```json
{
  "mcpServers": {
    "data-extractor": {
      "command": "uv",
      "args": ["run", "python", "-m", "extractor.server"],
      "cwd": "/path/to/your/data-extractor"
    }
  }
}
```

**æ³¨æ„äº‹é¡¹ï¼š**

- å°† `cwd` è·¯å¾„æ›¿æ¢ä¸ºæ‚¨çš„é¡¹ç›®å®é™…è·¯å¾„
- GitHub ä»“åº“åœ°å€ï¼š`https://github.com/ThreeFish-AI/data-extractor.git`
- æ¨èä½¿ç”¨æ–¹å¼äºŒï¼ˆæœ¬åœ° uv å¯åŠ¨ï¼‰è¿›è¡Œå¼€å‘ï¼Œæ–¹å¼ä¸‰ï¼ˆGitHub ç›´æ¥å®‰è£…ï¼‰ç”¨äºç”Ÿäº§ç¯å¢ƒ
- å½“å‰æœ€æ–°ç¨³å®šç‰ˆæœ¬ï¼šv0.1.2

## ğŸ› ï¸ å¯ç”¨å·¥å…·

### 1. scrape_webpage

åŸºç¡€ç½‘é¡µçˆ¬å–å·¥å…·ï¼Œæ”¯æŒå¤šç§æ–¹æ³•å’Œè‡ªå®šä¹‰æå–è§„åˆ™ã€‚

**å‚æ•°:**

- `url`: è¦çˆ¬å–çš„ URL
- `method`: çˆ¬å–æ–¹æ³• (auto/simple/scrapy/selenium)
- `extract_config`: æ•°æ®æå–é…ç½® (å¯é€‰)
- `wait_for_element`: ç­‰å¾…çš„ CSS é€‰æ‹©å™¨ (Selenium ä¸“ç”¨)

**ç¤ºä¾‹:**

```json
{
  "url": "https://example.com",
  "method": "auto",
  "extract_config": {
    "title": "h1",
    "content": {
      "selector": ".content p",
      "multiple": true,
      "attr": "text"
    }
  }
}
```

### 2. scrape_multiple_webpages

å¹¶å‘çˆ¬å–å¤šä¸ªç½‘é¡µã€‚

**ç¤ºä¾‹:**

```json
{
  "urls": ["https://example1.com", "https://example2.com"],
  "method": "simple",
  "extract_config": {
    "title": "h1",
    "links": "a"
  }
}
```

### 3. scrape_with_stealth

ä½¿ç”¨é«˜çº§åæ£€æµ‹æŠ€æœ¯çˆ¬å–ç½‘é¡µã€‚

**å‚æ•°:**

- `url`: ç›®æ ‡ URL
- `method`: éšèº«æ–¹æ³• (selenium/playwright)
- `extract_config`: æå–é…ç½®
- `wait_for_element`: ç­‰å¾…å…ƒç´ 
- `scroll_page`: æ˜¯å¦æ»šåŠ¨é¡µé¢åŠ è½½åŠ¨æ€å†…å®¹

**ç¤ºä¾‹:**

```json
{
  "url": "https://protected-site.com",
  "method": "playwright",
  "scroll_page": true,
  "wait_for_element": ".dynamic-content"
}
```

### 4. fill_and_submit_form

è¡¨å•å¡«å†™å’Œæäº¤ã€‚

**å‚æ•°:**

- `url`: åŒ…å«è¡¨å•çš„é¡µé¢ URL
- `form_data`: è¡¨å•å­—æ®µæ•°æ® (é€‰æ‹©å™¨:å€¼ å¯¹)
- `submit`: æ˜¯å¦æäº¤è¡¨å•
- `submit_button_selector`: æäº¤æŒ‰é’®é€‰æ‹©å™¨
- `method`: æ–¹æ³• (selenium/playwright)

**ç¤ºä¾‹:**

```json
{
  "url": "https://example.com/contact",
  "form_data": {
    "input[name='name']": "John Doe",
    "input[name='email']": "john@example.com",
    "textarea[name='message']": "Hello world"
  },
  "submit": true,
  "method": "selenium"
}
```

### 5. extract_links

ä¸“é—¨çš„é“¾æ¥æå–å·¥å…·ã€‚

**å‚æ•°:**

- `url`: ç›®æ ‡ URL
- `filter_domains`: åªåŒ…å«è¿™äº›åŸŸåçš„é“¾æ¥
- `exclude_domains`: æ’é™¤è¿™äº›åŸŸåçš„é“¾æ¥
- `internal_only`: åªæå–å†…éƒ¨é“¾æ¥

**ç¤ºä¾‹:**

```json
{
  "url": "https://example.com",
  "internal_only": true
}
```

### 6. extract_structured_data

è‡ªåŠ¨æå–ç»“æ„åŒ–æ•°æ® (è”ç³»ä¿¡æ¯ã€ç¤¾äº¤åª’ä½“é“¾æ¥ç­‰)ã€‚

**å‚æ•°:**

- `url`: ç›®æ ‡ URL
- `data_type`: æ•°æ®ç±»å‹ (all/contact/social/content)

**ç¤ºä¾‹:**

```json
{
  "url": "https://company.com",
  "data_type": "contact"
}
```

### 7. get_page_info

å¿«é€Ÿè·å–é¡µé¢åŸºç¡€ä¿¡æ¯ã€‚

**ç¤ºä¾‹:**

```json
{
  "url": "https://example.com"
}
```

### 8. check_robots_txt

æ£€æŸ¥ç½‘ç«™çš„ robots.txt æ–‡ä»¶ã€‚

### 9. get_server_metrics

è·å–æœåŠ¡å™¨æ€§èƒ½æŒ‡æ ‡å’Œç»Ÿè®¡ä¿¡æ¯ã€‚

### 10. clear_cache

æ¸…é™¤ç¼“å­˜çš„çˆ¬å–ç»“æœã€‚

## ğŸ“– æ•°æ®æå–é…ç½®

### ç®€å•é€‰æ‹©å™¨

```json
{
  "title": "h1",
  "links": "a"
}
```

### é«˜çº§é…ç½®

```json
{
  "products": {
    "selector": ".product",
    "multiple": true,
    "attr": "text"
  },
  "prices": {
    "selector": ".price",
    "multiple": true,
    "attr": "data-price"
  },
  "description": {
    "selector": ".description",
    "multiple": false,
    "attr": "text"
  }
}
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„æ–¹æ³•

- **simple**: é™æ€å†…å®¹ï¼Œå¿«é€Ÿçˆ¬å–
- **scrapy**: å¤§è§„æ¨¡çˆ¬å–ï¼Œéœ€è¦é«˜çº§ç‰¹æ€§
- **selenium**: JavaScript é‡åº¦ç½‘ç«™
- **stealth**: æœ‰åçˆ¬ä¿æŠ¤çš„ç½‘ç«™

### 2. éµå®ˆç½‘ç«™è§„åˆ™

- ä½¿ç”¨ `check_robots_txt` å·¥å…·æ£€æŸ¥çˆ¬å–è§„åˆ™
- è®¾ç½®åˆé€‚çš„å»¶è¿Ÿå’Œå¹¶å‘é™åˆ¶
- å°Šé‡ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾

### 3. æ€§èƒ½ä¼˜åŒ–

- ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è¯·æ±‚
- åˆç†è®¾ç½®è¶…æ—¶æ—¶é—´
- ç›‘æ§ `get_server_metrics` è°ƒæ•´é…ç½®

### 4. é”™è¯¯å¤„ç†

- å®æ–½é‡è¯•é€»è¾‘
- ç›‘æ§é”™è¯¯ç±»åˆ«
- æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­–ç•¥

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. Selenium/Playwright å¯åŠ¨å¤±è´¥**

- ç¡®ä¿å®‰è£…äº† Chrome æµè§ˆå™¨
- æ£€æŸ¥ç³»ç»Ÿæƒé™å’Œé˜²ç«å¢™è®¾ç½®

**2. åçˆ¬è™«æ£€æµ‹**

- ä½¿ç”¨ `scrape_with_stealth` å·¥å…·
- å¯ç”¨éšæœº User-Agent
- é…ç½®ä»£ç†æœåŠ¡å™¨

**3. è¶…æ—¶é”™è¯¯**

- å¢åŠ  `browser_timeout` è®¾ç½®
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ä½¿ç”¨æ›´ç¨³å®šçš„çˆ¬å–æ–¹æ³•

**4. å†…å­˜å ç”¨è¿‡é«˜**

- å‡å°‘å¹¶å‘è¯·æ±‚æ•°
- æ¸…ç†ç¼“å­˜
- æ£€æŸ¥æ˜¯å¦æœ‰èµ„æºæ³„éœ²

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

ä½¿ç”¨ `get_server_metrics` å·¥å…·ç›‘æ§ï¼š

- è¯·æ±‚æ€»æ•°å’ŒæˆåŠŸç‡
- å¹³å‡å“åº”æ—¶é—´
- é”™è¯¯åˆ†ç±»ç»Ÿè®¡
- æ–¹æ³•ä½¿ç”¨åˆ†å¸ƒ
- ç¼“å­˜å‘½ä¸­ç‡

## ğŸ”’ å®‰å…¨æ³¨æ„äº‹é¡¹

- ä¸è¦åœ¨æ—¥å¿—ä¸­è®°å½•æ•æ„Ÿä¿¡æ¯
- ä½¿ç”¨ HTTPS ä»£ç†æœåŠ¡å™¨
- å®šæœŸæ›´æ–°ä¾èµ–åŒ…
- éµå®ˆæ•°æ®ä¿æŠ¤æ³•è§„

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ã€‚

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ LICENSE æ–‡ä»¶

## ğŸ“ æ”¯æŒ

å¦‚é‡é—®é¢˜è¯·æäº¤ GitHub Issue æˆ–è”ç³»ç»´æŠ¤å›¢é˜Ÿã€‚

---

**æ³¨æ„**: è¯·è´Ÿè´£ä»»åœ°ä½¿ç”¨æ­¤å·¥å…·ï¼Œéµå®ˆç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾å’Œ robots.txt è§„åˆ™ï¼Œå°Šé‡ç½‘ç«™çš„çŸ¥è¯†äº§æƒã€‚
