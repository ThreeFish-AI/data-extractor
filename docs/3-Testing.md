---
id: testing
sidebar_position: 3
title: Testing
description: æœ¬æ–‡æ¡£æä¾› Data Extractor é¡¹ç›®çš„å®Œæ•´æµ‹è¯•æŒ‡å—ï¼Œæ¶µç›–æµ‹è¯•æ¶æ„ã€æ‰§è¡Œæ–¹æ³•ã€è´¨é‡ä¿éšœå’Œæ•…éšœæ’é™¤ç­‰å…¨æ–¹ä½å†…å®¹ã€‚
last_update:
  author: Aurelius
  date: 2025-11-23
tags:
  - Testing
  - Quality Assurance
  - Test Coverage
  - CI/CD
---

## ğŸ¯ æµ‹è¯•ä½“ç³»æ¦‚è¿°

### æ ¸å¿ƒç‰¹ç‚¹ä¸æ¶æ„åŸåˆ™

Data Extractor é¡¹ç›®å»ºç«‹äº†å®Œæ•´çš„æµ‹è¯•ä½“ç³»ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œç³»ç»Ÿç¨³å®šæ€§ï¼š

- **é«˜è¦†ç›–ç‡**ï¼šæ•´ä½“æµ‹è¯•è¦†ç›–ç‡è¾¾ 98%+
- **å¤šå±‚æ¬¡æµ‹è¯•**ï¼šå•å…ƒæµ‹è¯• â†’ é›†æˆæµ‹è¯• â†’ ç«¯åˆ°ç«¯æµ‹è¯•
- **è‡ªåŠ¨åŒ–æ‰§è¡Œ**ï¼šå®Œæ•´çš„ CI/CD é›†æˆå’Œè‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹
- **è¯¦ç»†æŠ¥å‘Š**ï¼šHTMLã€JSONã€XML ç­‰å¤šæ ¼å¼æµ‹è¯•æŠ¥å‘Š
- **æ€§èƒ½ç›‘æ§**ï¼šå†…å­˜ä½¿ç”¨ã€å“åº”æ—¶é—´ã€å¹¶å‘æ€§èƒ½ç›‘æ§

### æµ‹è¯•é‡‘å­—å¡”ä¸æ ‡è®°ä½“ç³»

**æµ‹è¯•ç±»å‹åˆ†å¸ƒ**ï¼š

| æµ‹è¯•ç±»å‹       | å æ¯” | ç›®æ ‡             | ç‰¹ç‚¹                |
| -------------- | ---- | ---------------- | ------------------- |
| **å•å…ƒæµ‹è¯•**   | 70%  | æµ‹è¯•å•ä¸ªå‡½æ•°å’Œç±» | å¿«é€Ÿæ‰§è¡Œã€Mock éš”ç¦» |
| **é›†æˆæµ‹è¯•**   | 25%  | æµ‹è¯•æ¨¡å—é—´äº¤äº’   | çœŸå®ç»„ä»¶äº¤äº’        |
| **ç«¯åˆ°ç«¯æµ‹è¯•** | 5%   | æµ‹è¯•å®Œæ•´ä¸šåŠ¡æµç¨‹ | çœŸå®ç¯å¢ƒæ¨¡æ‹Ÿ        |

**æµ‹è¯•æ ‡è®°åˆ†ç±»**ï¼š

```python
@pytest.mark.unit           # å•å…ƒæµ‹è¯•
@pytest.mark.integration    # é›†æˆæµ‹è¯•
@pytest.mark.slow          # æ…¢é€Ÿæµ‹è¯•
@pytest.mark.requires_network   # éœ€è¦ç½‘ç»œè®¿é—®
@pytest.mark.requires_browser   # éœ€è¦æµè§ˆå™¨ç¯å¢ƒ
```

### æµ‹è¯•ç›®å½•ç»“æ„

```
tests/
â”œâ”€â”€ conftest.py                     # Pytest é…ç½®å’Œå…±äº« fixtures
â”œâ”€â”€ unit/                           # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_config.py              # é…ç½®ç³»ç»Ÿæµ‹è¯•
â”‚   â”œâ”€â”€ test_scraper.py             # ç½‘é¡µæŠ“å–å¼•æ“æµ‹è¯•
â”‚   â”œâ”€â”€ test_markdown_converter.py  # Markdown è½¬æ¢å™¨æµ‹è¯•
â”‚   â”œâ”€â”€ test_pdf_processor.py       # PDF å¤„ç†å™¨æµ‹è¯•
â”‚   â””â”€â”€ test_utils.py               # å·¥å…·ç±»æµ‹è¯•
â”œâ”€â”€ integration/                    # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_mcp_tools.py           # MCP å·¥å…·é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_comprehensive_integration.py # ç»¼åˆé›†æˆæµ‹è¯•
â”‚   â””â”€â”€ test_end_to_end_integration.py # ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
â””â”€â”€ reports/                        # æµ‹è¯•æŠ¥å‘Šå­˜å‚¨
    â”œâ”€â”€ test-report.html            # ä¸»æµ‹è¯•æŠ¥å‘Š
    â”œâ”€â”€ htmlcov/                    # è¦†ç›–ç‡ HTML æŠ¥å‘Š
    â””â”€â”€ *.json                      # å„ç±»æµ‹è¯•ç»“æœ
```

### æ¨¡å—è¦†ç›–æ¦‚è§ˆ

| æ¨¡å—               | å•å…ƒæµ‹è¯• | é›†æˆæµ‹è¯• | è¦†ç›–èŒƒå›´             |
| ------------------ | -------- | -------- | -------------------- |
| MCP Server         | âœ…       | âœ…       | 14 ä¸ªå·¥å…·å®Œæ•´è¦†ç›–    |
| Web Scraper        | âœ…       | âœ…       | æ‰€æœ‰çˆ¬å–æ–¹æ³•         |
| Markdown Converter | âœ…       | âœ…       | è½¬æ¢å’Œæ ¼å¼åŒ–åŠŸèƒ½     |
| PDF Processor      | âœ…       | âœ…       | å¤šå¼•æ“å¤„ç†           |
| Configuration      | âœ…       | âœ…       | æ‰€æœ‰é…ç½®é€‰é¡¹         |
| Utilities          | âœ…       | âœ…       | ç¼“å­˜ã€æŒ‡æ ‡ã€é”™è¯¯å¤„ç† |

## ğŸ—ï¸ æµ‹è¯•åˆ†ç±»ä¸å®ç°

### å•å…ƒæµ‹è¯• (Unit Tests)

å•å…ƒæµ‹è¯•ä¸“æ³¨äºæµ‹è¯•ç‹¬ç«‹æ¨¡å—å’Œç±»çš„åŠŸèƒ½ï¼Œå…·æœ‰å¿«é€Ÿæ‰§è¡Œã€ä½¿ç”¨ Mock éš”ç¦»å¤–éƒ¨ä¾èµ–ã€é«˜ä»£ç è¦†ç›–ç‡çš„ç‰¹ç‚¹ã€‚

**æµ‹è¯•èŒƒå›´**ï¼šé…ç½®ç³»ç»Ÿã€ç½‘é¡µæŠ“å–å™¨ã€PDF å¤„ç†å™¨ã€å·¥å…·ç±»ç­‰

**ç¤ºä¾‹ä»£ç **ï¼š

```python
import pytest
from unittest.mock import AsyncMock, patch
from extractor.config import settings

class TestDataExtractor:
    """æµ‹è¯•æ•°æ®æå–å™¨"""

    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        extractor = DataExtractor(settings)
        assert extractor.config is settings
        assert extractor._cache == {}

    @pytest.mark.asyncio
    async def test_extract_data_success(self):
        """æµ‹è¯•æˆåŠŸæ•°æ®æå–"""
        extractor = DataExtractor(settings)
        result = await extractor.extract_data("https://example.com")
        assert "url" in result and "data" in result
        assert result["success"] is True

    @pytest.mark.asyncio
    async def test_extract_data_invalid_url(self):
        """æµ‹è¯•æ— æ•ˆURL"""
        extractor = DataExtractor(settings)
        with pytest.raises(ValueError, match="URL cannot be empty"):
            await extractor.extract_data("")
```

### é›†æˆæµ‹è¯• (Integration Tests)

é›†æˆæµ‹è¯•éªŒè¯å¤šä¸ªç»„ä»¶ä¹‹é—´çš„åä½œï¼Œä½¿ç”¨çœŸå®ç»„ä»¶äº¤äº’ã€ç«¯åˆ°ç«¯å·¥ä½œæµéªŒè¯ï¼Œä¸“æ³¨äºç»„ä»¶é—´çš„æ•°æ®æµå’Œæ¥å£ã€‚

**æµ‹è¯•èŒƒå›´**ï¼šMCP å·¥å…·é›†æˆã€è·¨å·¥å…·åä½œã€çœŸå®åœºæ™¯æ¨¡æ‹Ÿ

**ç¤ºä¾‹ä»£ç **ï¼š

```python
import pytest
from extractor.server import app

@pytest.mark.integration
@pytest.mark.asyncio
async def test_web_scraping_integration():
    """æµ‹è¯•ç½‘é¡µæŠ“å–é›†æˆ"""
    result = await app.scrape_webpage(
        url="https://httpbin.org/html",
        extract_config={"title": "h1"}
    )
    assert result.success is True
    assert "data" in result
```

**é›†æˆæµ‹è¯•æœ€ä½³å®è·µ**ï¼š

- **ä½¿ç”¨çœŸå®ç»„ä»¶**ï¼šä¼˜å…ˆä½¿ç”¨çœŸå®ç»„ä»¶è¿›è¡Œæµ‹è¯•
- **æ¨¡æ‹Ÿå¤–éƒ¨ä¾èµ–**ï¼šå¯¹å¤–éƒ¨ä¸å¯æ§ä¾èµ–ä½¿ç”¨ Mock
- **èµ„æºæ¸…ç†**ï¼šç¡®ä¿æµ‹è¯•åæ¸…ç†ä¸´æ—¶èµ„æºå’ŒçŠ¶æ€
- **ç¯å¢ƒéš”ç¦»**ï¼šä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•ç¯å¢ƒé¿å…äº¤å‰å½±å“

### æ€§èƒ½æµ‹è¯•ä¸ç«¯åˆ°ç«¯æµ‹è¯•

**æ€§èƒ½æµ‹è¯• (Performance Tests)**ï¼š

- **ç›®æ ‡**ï¼šè¯„ä¼°ç³»ç»Ÿåœ¨è´Ÿè½½ä¸‹çš„è¡¨ç°
- **å†…å®¹**ï¼šå¹¶å‘å¤„ç†æµ‹è¯•ã€å†…å­˜ä½¿ç”¨ç›‘æ§ã€å“åº”æ—¶é—´éªŒè¯
- **èŒƒå›´**ï¼šæ‰¹é‡å¤„ç†ã€å¤§æ–‡æ¡£å¤„ç†ã€é•¿æœŸç¨³å®šæ€§

**ç«¯åˆ°ç«¯æµ‹è¯• (E2E Tests)**ï¼š

- **ç›®æ ‡**ï¼šéªŒè¯å®Œæ•´çš„ä¸šåŠ¡æµç¨‹
- **å†…å®¹**ï¼šçœŸå®ç¯å¢ƒæ¨¡æ‹Ÿã€å®Œæ•´å·¥ä½œæµæµ‹è¯•
- **èŒƒå›´**ï¼šæ–‡æ¡£å¤„ç†ç®¡é“ã€é”™è¯¯æ¢å¤ã€æ•°æ®ä¸€è‡´æ€§

## ğŸš€ æµ‹è¯•æ‰§è¡Œä¸é…ç½®

### å¿«é€Ÿå¼€å§‹æŒ‡å—

**ä½¿ç”¨æµ‹è¯•è„šæœ¬**ï¼šé¡¹ç›®æä¾›äº†ç»¼åˆçš„æµ‹è¯•è¿è¡Œè„šæœ¬ `scripts/run-tests.sh`ï¼š

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶ï¼ˆé»˜è®¤ï¼‰
./scripts/run-tests.sh

# æŒ‰ç±»å‹è¿è¡Œæµ‹è¯•
./scripts/run-tests.sh unit           # å•å…ƒæµ‹è¯•
./scripts/run-tests.sh integration    # é›†æˆæµ‹è¯•
./scripts/run-tests.sh quick          # å¿«é€Ÿæµ‹è¯•ï¼ˆæ’é™¤æ…¢é€Ÿæµ‹è¯•ï¼‰
./scripts/run-tests.sh performance    # æ€§èƒ½æµ‹è¯•
./scripts/run-tests.sh clean          # æ¸…ç†æµ‹è¯•ç»“æœ
./scripts/run-tests.sh help           # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
```

**åŸºç¡€ pytest å‘½ä»¤**ï¼š

```bash
# è¿è¡Œä¸åŒèŒƒå›´çš„æµ‹è¯•
uv run pytest                              # æ‰€æœ‰æµ‹è¯•
uv run pytest tests/unit/                  # å•å…ƒæµ‹è¯•ç›®å½•
uv run pytest tests/integration/           # é›†æˆæµ‹è¯•ç›®å½•
uv run pytest tests/unit/test_config.py    # ç‰¹å®šæ–‡ä»¶

# è¿è¡Œç‰¹å®šæµ‹è¯•ç±»æˆ–æ–¹æ³•
uv run pytest tests/unit/test_config.py::TestDataExtractorSettings
uv run pytest tests/unit/test_config.py::TestDataExtractorSettings::test_default_settings
```

### é«˜çº§æµ‹è¯•é€‰é¡¹ä¸é…ç½®

**æµ‹è¯•æ‰§è¡Œæ§åˆ¶**ï¼š

```bash
# è¾“å‡ºæ§åˆ¶
uv run pytest -v                           # è¯¦ç»†è¾“å‡º
uv run pytest -s                           # æ˜¾ç¤º print ä¿¡æ¯
uv run pytest --tb=short                   # ç®€çŸ­é”™è¯¯ä¿¡æ¯
uv run pytest --tb=long                    # è¯¦ç»†é”™è¯¯ä¿¡æ¯

# æµ‹è¯•é€‰æ‹©ä¸æ‰§è¡Œ
uv run pytest -x                           # é¦–æ¬¡å¤±è´¥æ—¶åœæ­¢
uv run pytest --lf                         # åªè¿è¡Œä¸Šæ¬¡å¤±è´¥çš„æµ‹è¯•
uv run pytest --ff                         # å…ˆè¿è¡Œå¤±è´¥çš„æµ‹è¯•
uv run pytest --durations=10               # æ˜¾ç¤ºæœ€æ…¢çš„10ä¸ªæµ‹è¯•

# å¹¶è¡Œæ‰§è¡Œ
uv run pytest -n auto                      # è‡ªåŠ¨å¹¶è¡Œ
uv run pytest -n 4                         # 4ä¸ªè¿›ç¨‹å¹¶è¡Œ

# æŒ‰æ ‡è®°è¿è¡Œ
uv run pytest -m unit                      # å•å…ƒæµ‹è¯•
uv run pytest -m integration               # é›†æˆæµ‹è¯•
uv run pytest -m "not slow"                # æ’é™¤æ…¢é€Ÿæµ‹è¯•
uv run pytest -m requires_network          # éœ€è¦ç½‘ç»œçš„æµ‹è¯•
uv run pytest -m requires_browser          # éœ€è¦æµè§ˆå™¨çš„æµ‹è¯•
```

**æŠ¥å‘Šç”Ÿæˆ**ï¼š

```bash
# HTML æŠ¥å‘Š
uv run pytest --html=tests/reports/test-report.html --self-contained-html

# è¦†ç›–ç‡æŠ¥å‘Š
uv run pytest --cov=extractor --cov-report=html:tests/reports/htmlcov
uv run pytest --cov=extractor --cov-report=term-missing

# XML æŠ¥å‘Šï¼ˆCI/CD é›†æˆï¼‰
uv run pytest --junitxml=tests/reports/junit-results.xml
uv run coverage xml -o tests/reports/coverage.xml

# JSON æŠ¥å‘Š
uv run pytest --json-report --json-report-file=tests/reports/test-results.json
```

**Pytest é…ç½® (pyproject.toml)**ï¼š

```toml
[tool.pytest.ini_options]
minversion = "8.0"
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
testpaths = ["tests"]

# æµ‹è¯•æ ‡è®°å®šä¹‰
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "requires_network: marks tests that require network access",
    "requires_browser: marks tests that require browser setup"
]

# å¼‚æ­¥æ”¯æŒ
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"

# æ—¥å¿—é…ç½®
log_cli = true
log_cli_level = "INFO"
log_cli_format = "%(asctime)s [%(levelname)8s] %(name)s: %(message)s"
```

**è¦†ç›–ç‡é…ç½®**ï¼š

```toml
[tool.coverage.run]
source = ["extractor"]
omit = [
    "extractor/__init__.py",
    "tests/*",
    "venv/*",
    ".venv/*"
]
branch = true
parallel = true

[tool.coverage.report]
show_missing = true
skip_covered = false
sort = "Cover"
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "raise NotImplementedError",
    "if __name__ == .__main__.:"
]
```

## ğŸ”§ æµ‹è¯•ç¯å¢ƒä¸è°ƒè¯•

### ç¯å¢ƒå‡†å¤‡ä¸é…ç½®

**ç¯å¢ƒè®¾ç½®**ï¼š

```bash
# å®‰è£…å¼€å‘ä¾èµ–
uv sync --extra dev

# è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
export DATA_EXTRACTOR_ENABLE_JAVASCRIPT=false
export DATA_EXTRACTOR_CONCURRENT_REQUESTS=1
export DATA_EXTRACTOR_BROWSER_TIMEOUT=10
```

**æµ‹è¯•æ•°æ®å‡†å¤‡**ï¼š

```python
# ä½¿ç”¨æµ‹è¯•å¤¹å…·å‡†å¤‡æµ‹è¯•æ•°æ®
@pytest.fixture
def sample_html():
    """æ ‡å‡† HTML æµ‹è¯•å†…å®¹"""
    return """
    <!DOCTYPE html>
    <html>
    <head><title>Test Page</title></head>
    <body>
        <h1>Test Heading</h1>
        <div class="content">
            <p>Test paragraph 1</p>
            <p>Test paragraph 2</p>
        </div>
    </body>
    </html>
    """
```

### è°ƒè¯•æ–¹æ³•ä¸å·¥å…·

**è¾“å‡ºè°ƒè¯•**ï¼š

```bash
# æ˜¾ç¤ºè¯¦ç»†è¾“å‡ºå’Œæ‰“å°ä¿¡æ¯
uv run pytest -v -s

# æ§åˆ¶é”™è¯¯ä¿¡æ¯æ˜¾ç¤º
uv run pytest --tb=short    # ç®€çŸ­é”™è¯¯ä¿¡æ¯
uv run pytest --tb=long     # è¯¦ç»†é”™è¯¯ä¿¡æ¯
uv run pytest --tb=no       # ä¸æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
```

**äº¤äº’å¼è°ƒè¯•**ï¼š

```bash
# ä½¿ç”¨ PDB è°ƒè¯•å™¨
uv run pytest --pdb

# ä½¿ç”¨ IPython è°ƒè¯•å™¨
uv run pytest --pdb --pdbcls=IPython.terminal.debugger:Pdb

# åœ¨ç‰¹å®šæµ‹è¯•å¤„è®¾ç½®æ–­ç‚¹
def test_debug_example():
    import pdb; pdb.set_trace()  # è®¾ç½®æ–­ç‚¹
    assert True
```

**æ€§èƒ½ä¸å†…å­˜åˆ†æ**ï¼š

```bash
# å†…å­˜ä½¿ç”¨åˆ†æ
uv run pytest --monitor

# æ€§èƒ½åŸºå‡†æµ‹è¯•
uv run pytest --benchmark-only

# æ˜¾ç¤ºæµ‹è¯•æ‰§è¡Œæ—¶é—´
uv run pytest --durations=0
```

### æµ‹è¯•å¤¹å…·ä¸æ•°æ®ç®¡ç†

**æ ¸å¿ƒå¤¹å…·ç¤ºä¾‹**ï¼š

```python
@pytest.fixture
def test_config():
    """å®‰å…¨çš„æµ‹è¯•é…ç½®"""
    return DataExtractorSettings(
        server_name="Test Data Extractor",
        enable_javascript=False,
        concurrent_requests=1,
        browser_timeout=10,
        max_retries=2,
    )

@pytest.fixture
def mock_web_scraper():
    """æ¨¡æ‹Ÿ WebScraper å®ä¾‹"""
    scraper = Mock(spec=WebScraper)
    scraper.scrape_url = AsyncMock()
    return scraper

@pytest.fixture
async def sample_response():
    """æ¨¡æ‹Ÿ HTTP å“åº”"""
    class MockResponse:
        def __init__(self):
            self.status_code = 200
            self.text = "<html><body>Test</body></html>"

        async def json(self):
            return {"data": "test"}

    return MockResponse()
```

**æµ‹è¯•æ•°æ®ç®¡ç†**ï¼š

```python
# å¤–éƒ¨æµ‹è¯•æ•°æ®åŠ è½½
def load_test_data(filename):
    """åŠ è½½å¤–éƒ¨æµ‹è¯•æ•°æ®æ–‡ä»¶"""
    data_dir = Path(__file__).parent / "fixtures" / "data"
    return (data_dir / filename).read_text()

# ä½¿ç”¨å‚æ•°åŒ–æµ‹è¯•
@pytest.mark.parametrize("url,expected_status", [
    ("https://httpbin.org/html", 200),
    ("https://httpbin.org/json", 200),
])
async def test_url_status(url, expected_status):
    """å‚æ•°åŒ–æµ‹è¯•ç¤ºä¾‹"""
    response = await fetch_url(url)
    assert response.status_code == expected_status
```

## ğŸ“Š æµ‹è¯•æŠ¥å‘Šä¸è´¨é‡åˆ†æ

### æŠ¥å‘Šç”Ÿæˆä¸è´¨é‡æŒ‡æ ‡

**HTML æŠ¥å‘Š**ï¼š

```bash
# ç”Ÿæˆä¸»æµ‹è¯•æŠ¥å‘Š
uv run pytest --html=tests/reports/test-report.html --self-contained-html

# æŸ¥çœ‹æŠ¥å‘Šï¼ˆæ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©ï¼‰
open tests/reports/test-report.html          # macOS
xdg-open tests/reports/test-report.html      # Linux
start tests/reports/test-report.html         # Windows
```

**è¦†ç›–ç‡æŠ¥å‘Š**ï¼š

```bash
# ç”Ÿæˆ HTML è¦†ç›–ç‡æŠ¥å‘Š
uv run pytest --cov=extractor --cov-report=html:tests/reports/htmlcov

# æŸ¥çœ‹è¦†ç›–ç‡æŠ¥å‘Š
open tests/reports/htmlcov/index.html

# ç»ˆç«¯è¦†ç›–ç‡æ˜¾ç¤º
uv run coverage report --show-missing
```

**å¤šæ ¼å¼æŠ¥å‘Š**ï¼š

```bash
# JSON æŠ¥å‘Šï¼ˆä¾¿äº CI/CD é›†æˆï¼‰
uv run pytest --json-report --json-report-file=tests/reports/test-results.json

# JUnit XML æŠ¥å‘Šï¼ˆCI/CD æ ‡å‡†ï¼‰
uv run pytest --junitxml=tests/reports/junit-results.xml

# Cobertura XML è¦†ç›–ç‡æŠ¥å‘Š
uv run coverage xml -o tests/reports/coverage.xml

# ç‰ˆæœ¬å¯¹æ¯”æŠ¥å‘Š
mkdir -p reports/v$(cat version.txt 2>/dev/null || echo "latest")
cp reports/*.{html,json,xml} reports/v$(cat version.txt 2>/dev/null || echo "latest")/
```

**è´¨é‡æŒ‡æ ‡ç¤ºä¾‹**ï¼š

```json
{
  "summary": {
    "total": 219,
    "passed": 216,
    "failed": 0,
    "skipped": 3,
    "duration": 47.92,
    "success_rate": "98.6%"
  },
  "coverage": {
    "total": "98%",
    "lines": "98%",
    "branches": "95%",
    "functions": "100%"
  }
}
```

## ğŸ† æµ‹è¯•è´¨é‡ä¿éšœä¸æœ€ä½³å®è·µ

### æµ‹è¯•è®¾è®¡åŸåˆ™

**æ ¸å¿ƒåŸåˆ™**ï¼š

- **ç‹¬ç«‹æ€§**ï¼šæ¯ä¸ªæµ‹è¯•åº”è¯¥ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¾èµ–å…¶ä»–æµ‹è¯•
- **å¯é‡å¤æ€§**ï¼šæµ‹è¯•ç»“æœåº”è¯¥æ˜¯å¯é‡å¤çš„ï¼Œä¸å—ç¯å¢ƒå½±å“
- **å¿«é€Ÿæ‰§è¡Œ**ï¼šå•å…ƒæµ‹è¯•åº”è¯¥å¿«é€Ÿæ‰§è¡Œï¼Œé›†æˆæµ‹è¯•å¯ä»¥ç¨æ…¢
- **æ¸…æ™°æ–­è¨€**ï¼šä½¿ç”¨æ˜ç¡®çš„æ–­è¨€ï¼Œæä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯

**å‘½åè§„èŒƒä¸ç»“æ„**ï¼š

```python
def test_function_behavior_condition():
    """æµ‹è¯•å‡½æ•°åœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„è¡Œä¸º"""
    pass

class TestClassName:
    """æµ‹è¯•ç±»åç§°è§„èŒƒ"""

    def test_method_scenario(self):
        """æµ‹è¯•æ–¹æ³•åœºæ™¯"""
        pass
```

**AAA æµ‹è¯•æ¨¡å¼**ï¼š

```python
def test_example():
    # Arrange - å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data = {"key": "value"}

    # Act - æ‰§è¡Œè¢«æµ‹è¯•çš„æ“ä½œ
    result = function_under_test(test_data)

    # Assert - éªŒè¯ç»“æœ
    assert result["success"] is True
```

**å¼‚å¸¸æµ‹è¯•æ¨¡å¼**ï¼š

```python
def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    with pytest.raises(ValueError, match="Invalid input"):
        function_with_validation("invalid_input")
```

### é«˜çº§æµ‹è¯•æŠ€å·§ä¸ç­–ç•¥

**Mock ä¸ Patch ç­–ç•¥**ï¼š

```python
from unittest.mock import Mock, AsyncMock, patch

def test_with_mock():
    # Mock å¤–éƒ¨ä¾èµ–
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"data": "test"}

    with patch('requests.get', return_value=mock_response):
        result = function_that_uses_requests()
        assert result is not None

# å¼‚æ­¥ Mock
@pytest.mark.asyncio
async def test_async_mock():
    mock_async_func = AsyncMock(return_value={"result": "success"})
    result = await mock_async_func()
    assert result["result"] == "success"
```

**å‚æ•°åŒ–æµ‹è¯•**ï¼š

```python
@pytest.mark.parametrize("input_url,expected", [
    ("https://example.com", True),
    ("http://test.org", True),
    ("not-a-url", False),
    ("", False),
])
def test_url_validation(input_url, expected):
    validator = URLValidator()
    assert validator.is_valid(input_url) == expected

# å¤æ‚å‚æ•°åŒ–æµ‹è¯•
@pytest.mark.parametrize("method,status_code,expected", [
    ("GET", 200, "success"),
    ("POST", 201, "created"),
    ("DELETE", 204, "no_content"),
])
async def test_http_responses(method, status_code, expected):
    response = await make_request(method, "/test")
    assert response.status_code == status_code
    assert response.message == expected
```

**æµ‹è¯•æ•°æ®ç®¡ç†**ï¼š

```python
# ä½¿ç”¨ fixtures ç®¡ç†æµ‹è¯•æ•°æ®
@pytest.fixture(params=["simple", "complex", "edge_case"])
def test_scenario(request):
    """å‚æ•°åŒ–å¤¹å…·"""
    scenarios = {
        "simple": {"url": "https://example.com", "expected": "success"},
        "complex": {"url": "https://complex-site.com", "expected": "partial"},
        "edge_case": {"url": "", "expected": "error"}
    }
    return scenarios[request.param]

# å¤–éƒ¨æ•°æ®æ–‡ä»¶ç®¡ç†
def load_test_cases():
    """ä» JSON æ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
    with open("tests/fixtures/test_cases.json") as f:
        return json.load(f)
```

## ğŸ”„ CI/CD é›†æˆä¸è‡ªåŠ¨åŒ–

### GitHub Actions åŸºç¡€ CI é…ç½®

```yaml
name: CI

on:
  push:
    branches: [master, main, develop]
  pull_request:
    branches: [master, main, develop]

jobs:
  test:
    name: Test Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.12", "3.13"]

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run tests
        run: uv run pytest -v --tb=short --cov=extractor
        env:
          DATA_EXTRACTOR_ENABLE_JAVASCRIPT: false

      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

### é«˜çº§ CI/CD å·¥ä½œæµ

```yaml
name: Advanced CI/CD

on:
  push:
    branches: [master, main]
  pull_request:
    branches: [master, main]

jobs:
  test-matrix:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.12", "3.13"]
        test-type: [unit, integration, performance]

    steps:
      - uses: actions/checkout@v4
      - name: Setup Environment
        run: |
          uv sync --extra dev
          echo "DATA_EXTRACTOR_ENABLE_JAVASCRIPT=false" >> $GITHUB_ENV

      - name: Run ${{ matrix.test-type }} tests
        run: |
          if [ "${{ matrix.test-type }}" = "unit" ]; then
            uv run pytest -m unit --cov=extractor
          elif [ "${{ matrix.test-type }}" = "integration" ]; then
            uv run pytest -m integration -v
          elif [ "${{ matrix.test-type }}" = "performance" ]; then
            uv run pytest -m performance --benchmark-only
          fi

  quality-gate:
    needs: test-matrix
    runs-on: ubuntu-latest
    steps:
      - name: Quality Gate Check
        run: |
          # æ£€æŸ¥è¦†ç›–ç‡ã€æ€§èƒ½ç­‰è´¨é‡æŒ‡æ ‡
          python scripts/check_quality_gate.py
```

### æœ¬åœ° Pre-commit Hooks

```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: run-tests
        name: Run tests
        entry: uv run pytest -m "not slow"
        language: system
        pass_filenames: false
        always_run: true

      - id: check-coverage
        name: Check coverage
        entry: uv run pytest --cov=extractor --cov-fail-under=95
        language: system
        pass_filenames: false

      - id: lint-code
        name: Lint code
        entry: uv run ruff check .
        language: system
        always_run: true
```

```bash
# å®‰è£…å’Œä½¿ç”¨ pre-commit
pip install pre-commit
pre-commit install
pre-commit run --all-files  # æ‰‹åŠ¨è¿è¡Œ
```

### è´¨é‡é—¨ç¦æ ‡å‡†

| æŒ‡æ ‡           | è¦æ±‚    | å½“å‰çŠ¶æ€ | é€šè¿‡æƒ…å†µ |
| -------------- | ------- | -------- | -------- |
| å•å…ƒæµ‹è¯•é€šè¿‡ç‡ | >99%    | 99.2%    | âœ…       |
| é›†æˆæµ‹è¯•é€šè¿‡ç‡ | >95%    | 98.6%    | âœ…       |
| æ€»ä½“æµ‹è¯•é€šè¿‡ç‡ | >98%    | 98.6%    | âœ…       |
| ä»£ç è¦†ç›–ç‡     | >95%    | 98%      | âœ…       |
| æµ‹è¯•æ‰§è¡Œæ—¶é—´   | <5 åˆ†é’Ÿ | ~3 åˆ†é’Ÿ  | âœ…       |
| å†…å­˜ä½¿ç”¨       | <1GB    | ~512MB   | âœ…       |
| å¹¶å‘å¤„ç†èƒ½åŠ›   | 20 ä»»åŠ¡ | é€šè¿‡     | âœ…       |

## ğŸš¨ æ•…éšœæ’é™¤ä¸ç»´æŠ¤

### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

**æµè§ˆå™¨ç›¸å…³é—®é¢˜**ï¼š

```bash
# Selenium æˆ– Playwright æµ‹è¯•å¤±è´¥è§£å†³æ–¹æ¡ˆ
# æ–¹æ¡ˆ1ï¼šç¦ç”¨æµè§ˆå™¨æµ‹è¯•
export DATA_EXTRACTOR_ENABLE_JAVASCRIPT=false
uv run pytest -k "not requires_browser"

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨ headless æ¨¡å¼
export PLAYWRIGHT_BROWSERS_PATH=0
uv run playwright install --with-deps
uv run pytest --browser chromium --headless
```

**ç½‘ç»œè¿æ¥é—®é¢˜**ï¼š

```bash
# ç½‘ç»œè¶…æ—¶æˆ–è¿æ¥å¤±è´¥è§£å†³æ–¹æ¡ˆ
# è·³è¿‡ç½‘ç»œä¾èµ–æµ‹è¯•
uv run pytest -k "not requires_network"

# ä½¿ç”¨ Mock æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚
uv run pytest --mock-network

# å¢åŠ è¶…æ—¶è®¾ç½®
uv run pytest --timeout=30
```

**å¼‚æ­¥æµ‹è¯•é—®é¢˜**ï¼š

```bash
# å¼‚æ­¥æµ‹è¯•è¶…æ—¶æˆ–å¤±è´¥è§£å†³æ–¹æ¡ˆ
# å¢åŠ å¼‚æ­¥è¶…æ—¶æ—¶é—´
uv run pytest --asyncio-mode=auto --timeout=60

# æ£€æŸ¥å¼‚æ­¥è£…é¥°å™¨ä½¿ç”¨
@pytest.mark.asyncio
async def test_async_function():
    result = await async_operation()
    assert result is not None
```

**æ€§èƒ½ä¸èµ„æºé—®é¢˜**ï¼š

```bash
# å†…å­˜ä¸è¶³æˆ–æ‰§è¡Œç¼“æ…¢è§£å†³æ–¹æ¡ˆ
# å‡å°‘å¹¶å‘æ•°é‡
uv run pytest -n 1

# æ¸…ç†æµ‹è¯•ç¼“å­˜
rm -rf /tmp/test_cache_*
rm -rf .pytest_cache/

# è·³è¿‡æ…¢é€Ÿæµ‹è¯•
uv run pytest -m "not slow"
```

### è°ƒè¯•ä¸æ€§èƒ½åˆ†æ

**æ—¥å¿—è°ƒè¯•**ï¼š

```python
import logging
logger = logging.getLogger(__name__)

def test_with_logging():
    logger.info("Starting test")
    result = some_operation()
    logger.debug(f"Result: {result}")
    assert result is not None
```

**æ–­ç‚¹è°ƒè¯•**ï¼š

```python
def test_debug_example():
    import pdb; pdb.set_trace()  # è®¾ç½®æ–­ç‚¹
    # æµ‹è¯•ä»£ç 
    assert True

# ä½¿ç”¨ IPython è°ƒè¯•å™¨
uv run pytest --pdb --pdbcls=IPython.terminal.debugger:Pdb
```

**æ€§èƒ½åˆ†æ**ï¼š

```python
import time
import pytest

@pytest.mark.slow
def test_performance_benchmark():
    start_time = time.time()
    result = expensive_operation()
    duration = time.time() - start_time

    assert duration < 5.0  # ç¡®ä¿æ‰§è¡Œæ—¶é—´åœ¨5ç§’å†…
    assert result is not None
```

### æµ‹è¯•æ•°æ®ä¸ç»´æŠ¤ç­–ç•¥

**æµ‹è¯•æ•°æ®ç»“æ„**ï¼š

```
tests/
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ html/           # HTML æµ‹è¯•æ–‡ä»¶
â”‚   â”œâ”€â”€ pdf/            # PDF æµ‹è¯•æ–‡ä»¶
â”‚   â””â”€â”€ data/           # JSON æµ‹è¯•æ•°æ®
â””â”€â”€ conftest.py         # å…±äº«å¤¹å…·
```

**æµ‹è¯•æ•°æ®ç®¡ç†**ï¼š

```python
# ä½¿ç”¨å¤¹å…·ç®¡ç†
@pytest.fixture
def sample_html():
    """æ ‡å‡† HTML æµ‹è¯•å†…å®¹"""
    return """<html><body><h1>Test</h1></body></html>"""

@pytest.fixture(params=["simple", "complex"])
def test_scenarios(request):
    """å‚æ•°åŒ–æµ‹è¯•åœºæ™¯"""
    return load_scenario(request.param)

# å¤–éƒ¨æ•°æ®åŠ è½½
def load_test_data(filename):
    """åŠ è½½å¤–éƒ¨æµ‹è¯•æ•°æ®æ–‡ä»¶"""
    data_dir = Path(__file__).parent / "fixtures" / "data"
    return (data_dir / filename).read_text()
```

**ç»´æŠ¤ä¸ä¼˜åŒ–ç­–ç•¥**ï¼š

**å®šæœŸç»´æŠ¤ä»»åŠ¡**ï¼š

- **æµ‹è¯•ç”¨ä¾‹æ›´æ–°**ï¼šå®šæœŸæ£€æŸ¥å’Œæ›´æ–°è¿‡æ—¶çš„æµ‹è¯•ç”¨ä¾‹
- **è¦†ç›–ç‡ç›‘æ§**ï¼šæŒç»­ç›‘æ§ä»£ç è¦†ç›–ç‡ï¼Œç¡®ä¿ä¸ä¸‹é™
- **æ€§èƒ½åŸºå‡†æ£€æŸ¥**ï¼šå»ºç«‹æ€§èƒ½åŸºå‡†å¹¶å®šæœŸæ£€æŸ¥å›å½’
- **ä¾èµ–æ›´æ–°**ï¼šå®šæœŸæ›´æ–°æµ‹è¯•ä¾èµ–å’Œå·¥å…·

**æµ‹è¯•ç»„ç»‡æœ€ä½³å®è·µ**ï¼š

- **åŠŸèƒ½åˆ†ç»„**ï¼šæŒ‰åŠŸèƒ½æ¨¡å—ç»„ç»‡æµ‹è¯•æ–‡ä»¶
- **æ ‡è®°åˆ†ç±»**ï¼šåˆç†ä½¿ç”¨ pytest æ ‡è®°è¿›è¡Œæµ‹è¯•åˆ†ç±»
- **å‘½åè§„èŒƒ**ï¼šä½¿ç”¨æ¸…æ™°ã€ä¸€è‡´çš„æµ‹è¯•å‘½åè§„èŒƒ
- **æ–‡æ¡£è¯´æ˜**ï¼šä¸ºå¤æ‚æµ‹è¯•æä¾›å……åˆ†çš„æ–‡æ¡£è¯´æ˜

**æŒç»­æ”¹è¿›**ï¼š

- **æµ‹è¯•åé¦ˆ**ï¼šæ”¶é›†å’Œåˆ†ææµ‹è¯•å¤±è´¥åé¦ˆ
- **è‡ªåŠ¨åŒ–å¢å¼º**ï¼šé€æ­¥æé«˜æµ‹è¯•è‡ªåŠ¨åŒ–ç¨‹åº¦
- **å·¥å…·ä¼˜åŒ–**ï¼šå®šæœŸè¯„ä¼°å’Œä¼˜åŒ–æµ‹è¯•å·¥å…·é“¾
- **å›¢é˜ŸåŸ¹è®­**ï¼šå®šæœŸè¿›è¡Œæµ‹è¯•æœ€ä½³å®è·µåŸ¹è®­

---

é€šè¿‡éµå¾ªæœ¬æµ‹è¯•æŒ‡å—ï¼Œå›¢é˜Ÿå¯ä»¥å»ºç«‹å’Œç»´æŠ¤é«˜è´¨é‡çš„æµ‹è¯•ä½“ç³»ï¼Œç¡®ä¿ Data Extractor é¡¹ç›®çš„ç¨³å®šæ€§ã€å¯é æ€§å’ŒæŒç»­æ”¹è¿›ã€‚
